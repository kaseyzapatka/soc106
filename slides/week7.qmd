---
title: "Week 7"
subtitle: "Sociology 106: Quantitative Sociological Methods"
date: 03/03/2026
date-format: long
format:
  revealjs:
    slide-number: true
    chalkboard:
      buttons: false
    preview-links: auto
    css: slides.css
    footer: '[GitHub course repo](https://www.kaseyzapatka.com/soc106)'
---

```{r}
#| echo: false
#| results: false

library(tidyverse)
library(here)
library(patchwork)

attain <- read_csv(here("data/attain.csv"))
```

## Housekeeping {.smaller}

[[**ADD IT IN LATER**]]

## Where We Are in the Course {.smaller}

- **Weeks 5–6:** rules of probability + probability models (Bernoulli, Binomial, Normal)
- **This week (Week 7):** sampling distributions and the Central Limit Theorem
- **Next week (Week 8):** confidence intervals
- **Then (Week 9):** hypothesis testing

**Main idea:** We've been building probability models of *populations*. Today we ask: what happens when we draw a *sample*—and then another, and another? The answer is what makes all of statistical inference possible.

## Agenda {.smaller}

**Admin discussion:**

- Research paper proposals and annotated bibliography

**Statistical content — three parts:**

- **Part 1**: Sampling — from populations to data
- **Part 2**: Sampling distributions — statistics vary from sample to sample
- **Part 3**: The Central Limit Theorem — the key that unlocks inference

## Research Paper Proposals {.smaller}

**Overall, great job—super interesting topics!**

I tried to give feedback in two areas:

1. **Research design**: How to construct variables of interest, what units of analysis may be most appropriate, other issues you may want to think about going forward

2. **What has been written in the academic literature** on your topic — some topics have been studied a lot, which is a **good sign** that your question is important!

**If your question seems already answered**: try a different data source, or look for related unanswered questions in paper conclusions. Happy to discuss in office hours!

## Annotated Bibliography {.smaller}

<span class="highlight">Due March 19</span>

Identify **ten scholarly sources** related to your research question

For each source: in **two short paragraphs**: 

1. summarize what the source is arguing
2. explain how it relates to your research question

::: {.panel-tabset}

### Good sources:

- Articles in academic journals
- Academic books
- Book chapters from edited volumes
- Non-political reports from research centers / government agencies

### Bad sources:

- Blogs, internet / newspaper articles
- Wikipedia pages
- Political reports from research centers / govt

:::

## How to Find Academic Articles {.smaller}

**Search on Google Scholar or JSTOR**

You'll usually get somewhere by taking your independent and dependent variables and searching for:

> "effect of *independent variable* on *dependent variable*"

You may need to use a [UC Berkeley Library proxy](https://search.library.berkeley.edu/discovery/search?vid=01UCS_BER:UCB) to access some academic articles

This is a [helpful webpage](https://guides.lib.berkeley.edu/ezproxy/home) for using the proxy server and here is a [link to make a virtual appointment](https://ucberkeley.libanswers.com/faq/302000) for research help

<span class="highlight">NOTE: AI will hallucinate citations, so don't use</span>

## How to Read Academic Articles {.smaller}

| Section | What to look for |
|-|---|
| **Abstract** | Overview: research question, data, main result — read first |
| **Introduction** | Slightly more detailed than abstract; same format |
| **Conclusion** | Results, alternative explanations, limitations — and **future research topics** |
| **Background / Lit review** | Previous research; a **good source of additional citations** |
| **Data section** | How the sample was constructed; what data sources researchers use |
| **Methods** | Don't worry about this too much yet! |

:::{.aside}
**Tip**: Start with abstract → conclusion → introduction. Only go deeper if the paper is clearly relevant.
:::

## Questions?

# Part 1: From Populations to Samples

How do we go from an abstract population to real data we can analyze?

## Sampling: Basic Concepts {.smaller}

| | **Population** | **Sample** |
|-|---|---|
| **Concept** | The whole universe a study aspires to generalize to | The subset of the population we actually observe |
| **Quantity** | **Parameter** — a number describing the population (usually unknown) | **Statistic** — a number computed from the data |
| **Example** | True proportion of Philadelphia residents who are African American: $\pi = 0.422$ | Proportion of stopped drivers who were African American: $\hat{p} = 0.79$ |

<br>
<span class="highlight">**The goal of statistical inference**</span>: use a sample *statistic* to learn about a population *parameter* — we've been building toward this all semester

## Sampling from a Population {.smaller}

We start with a **population** — every individual we want to study. In our running example, this is all 1.45 million residents of Philadelphia in 1997.

![](images/week7/pop_vs_sample0.png){width="75%" fig-align="center"}

## Sampling from a Population {.smaller}

A **sample** is drawn from the population — the individuals we actually observe. The 262 drivers stopped by police are our sample, far smaller than the full population.

![](images/week7/pop_vs_sample1.png){width="75%" fig-align="center"}

## Sampling from a Population {.smaller}

**Key question**: is our sample *representative*? Does it reflect the composition of the population? This depends entirely on the sampling design — random samples are representative; convenience samples often are not.

![](images/week7/pop_vs_sample2.png){width="75%" fig-align="center"}

## Sample Design {.smaller}

To do inferential statistics, we need a **representative sample** — ideally, each unit of the population has an equal chance of being included

We also want samples that are **large enough for precision**, which increases with sample size

<br> 

<span class="highlight">**Important note**:</span> Precision is inversely proportional to the diversity of values — larger samples are needed to draw inferences about small subgroups (by race, education, sexual orientation, etc.)


## Types of Sampling Designs {.smaller}

::: {.panel-tabset}

### Simple random

**Simple random sample:** Use a random number generator to select from a population list

![](images/week7/sampling_simple.png){width="80%" fig-align="center"}

### Stratified random

**Stratified random sample**: Divide population into homogenous groups (strata), then randomly sample within each stratum

- Generally improves precision
- Can over-sample smaller strata (e.g., minority groups) to make inferences *within* those groups

![](images/week7/sampling_stratified.png){width="65%" fig-align="center"}

### Cluster

**Cluster sampling**:

1. Create subpopulations (clusters)
2. Randomly select a sample of clusters
3. Randomly select units within sampled clusters

![](images/week7/sampling_cluster.png){width="65%" fig-align="center"}

:::

## Stratified vs. Cluster Sampling {.smaller}

These sound similar — the **key difference**: cluster sampling selects only *some* groups; stratified sampling samples from *all* strata.

![](images/week7/sampling_stratified.png){width="60%" fig-align="center"}

![](images/week7/sampling_cluster.png){width="60%" fig-align="center"}

## Representative Samples & Sample Weights {.smaller}

::: {.panel-tabset}

### The Problem

True random samples are almost never feasible — we rarely have a complete population list

**But** we often know the *probability* that each individual would be selected, based on demographics, geography, or other characteristics

### Sample Weights

**Weights** are set inversely proportional to the probability of selection:

- Individuals *less likely* to be sampled → *higher* weight
- Individuals *more likely* to be sampled → *lower* weight
- Result: a weighted sample that mirrors the full population

### GSS Example

The GSS uses cluster sampling. Within each selected household, only one adult is chosen:

- Individuals in *larger* households are *less* likely to be selected → *higher* weights
- Additional weights for oversampled Black respondents
- Additional weights for non-responders on the first contact

<span class="highlight">Always check whether your dataset uses weights, and apply them in R</span>

:::

# Part 2: Sampling Distributions

Statistics vary from sample to sample — they have their own distributions

## What is a Sampling Distribution? {.smaller .scrollable}

A **sampling distribution** is the probability distribution of a sample statistic computed across many independent samples from the same population

**Three distributions you must keep straight:**

| Distribution | What it describes | Philadelphia example |
|---|---|---|
| **Population** | All individuals in the population | 1.45M Philadelphia residents, 42.2% African American |
| **Sample (data)** | The individuals in *our* sample | 262 police stops in 1997 |
| **Sampling** | How our statistic would vary across *repeated* samples | Distribution of $\hat{p}$ across many random samples of 262 |

<span class="highlight">**Key insights:**</span>

- Sampling distributions describe **variability from study to study**
- They tell us **how close a statistic is likely to fall** to the true population parameter
- They are **not what we observe** — they represent what we *would* observe across many hypothetical repetitions
- They are the foundation for confidence intervals (Week 8) and hypothesis tests (Week 9)

## Sampling Distribution of a Proportion {.smaller}

::: {.panel-tabset}

### The bridge from Week 6

Recall: $X \sim B(n, \pi)$ has mean $n\pi$ and SD $\sqrt{n\pi(1-\pi)}$

The **sample proportion** $\hat{p} = X/n$ divides that count by $n$. Its sampling distribution:

$$\mu_{\hat{p}} = \frac{n\pi}{n} = \pi \qquad SE(\hat{p}) = \frac{\sqrt{n\pi(1-\pi)}}{n} = \sqrt{\frac{\pi(1-\pi)}{n}}$$

**Same formula as last week's Binomial SD — divided by $n$.**

### What it tells us

$$\mu_{\hat{p}} = \pi \qquad \text{(unbiased — centered on the true proportion)}$$

$$SE(\hat{p}) = \sqrt{\frac{\pi(1-\pi)}{n}} \qquad \text{(shrinks as } n \text{ grows)}$$

We use *standard error* (SE) rather than *standard deviation* to signal that this is the spread of a **sampling distribution**, not of raw data.

### Connection to inference

The SE answers: on average, how far will our $\hat{p}$ fall from the true $\pi$?

- Small $n$ → large SE → estimates can be far from the truth
- Large $n$ → small SE → estimates are reliably close to the truth

This is the mechanism that makes large surveys more trustworthy — and why the GSS (≈3,000 respondents) gives more reliable estimates than a sample of 20.

:::

## Racial Profiling: A Worked Example {.smaller}

::: {.panel-tabset}

### The Setup

**Data from Philadelphia, 1997:**

- **262** police car stops were recorded
- **207** of the stopped drivers were African American ($\hat{p} = 207/262 = 0.79$)
- Philadelphia's population at the time was **42.2% African American** ($\pi = 0.422$)

**Question**: If drivers were stopped at random, how likely is it that 79% of stopped drivers would be African American?

### The Math

Assume random stopping: $X \sim B(262, 0.422)$

The **sampling distribution** of $\hat{p}$ under random stopping:

$$\mu_{\hat{p}} = 0.422$$

$$SE(\hat{p}) = \sqrt{\frac{0.422 \times 0.578}{262}} \approx 0.030$$

### The Verdict

**Observed**: $\hat{p} = 0.79$

**Distance from expected**:

$$z = \frac{0.79 - 0.422}{0.030} \approx 12 \text{ standard errors above the mean}$$

Under random stopping, $\hat{p} = 0.79$ is essentially impossible — it lies 12 SEs above what we'd expect. The sampling distribution reveals that this pattern cannot be explained by chance, but some other bias. 

:::

## Visualizing the Sampling Distribution {.smaller}

As $N$ increases, $\hat{p}$ becomes **tighter and more bell-shaped** — the Central Limit Theorem in action:

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-height: 5
#| fig-align: center

set.seed(106)
n_sims       <- 5000
pi_val       <- 0.422
sample_sizes <- c(4, 8, 40, 400, 4000, 40000)

fmt_n <- function(n) formatC(n, format = "d", big.mark = ",")

# Pre-generate samples for all N in sequence (single seed)
all_draws <- map(sample_sizes, ~ rbinom(n_sims, size = .x, prob = pi_val) / .x)

make_panel <- function(n, draws) {
  df <- tibble(p_hat = draws)
  se <- sqrt(pi_val * (1 - pi_val) / n)

  hist_layer <- if (n <= 400) {
    geom_histogram(aes(y = after_stat(density)), binwidth = 1 / n,
                   boundary = 0, fill = "#4E79A7", color = "white",
                   linewidth = 0.15)
  } else {
    geom_histogram(aes(y = after_stat(density)), bins = 60,
                   fill = "#4E79A7", color = "white", linewidth = 0.15)
  }
  ggplot(df, aes(x = p_hat)) +
    hist_layer +
    stat_function(fun = dnorm, args = list(mean = pi_val, sd = se),
                  color = "#E15759", linewidth = 0.9) +
    labs(x = NULL, y = NULL,
         title = paste0("N = ", fmt_n(n))) +
    theme_minimal(base_size = 10) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5),
          axis.text  = element_text(size = 7))
}

panels <- map2(sample_sizes, all_draws, make_panel)

wrap_plots(panels, nrow = 2) +
  plot_annotation(
    title   = "Sampling distribution of p̂ as N grows  (π = 0.422)",
    #caption = "Red dashed line = true π  |  5,000 simulated samples per panel",
    theme   = theme(
      plot.title   = element_text(size = 12),
      plot.caption = element_text(color = "gray50", size = 8)
    )
  )
```

## Sampling Distribution of the Mean {.smaller}

::: {.panel-tabset}

### The Formula

For a random sample of size $n$ from a population with mean $\mu$ and standard deviation $\sigma$:

$$\mu_{\bar{x}} = \mu \qquad \text{(sample mean is unbiased)}$$

$$SE(\bar{x}) = \frac{\sigma}{\sqrt{n}} \qquad \text{(precision increases with sample size)}$$

Doubling $n$ cuts the SE by a factor of $\sqrt{2}$ — not 2. Precision is expensive!

### Worked Example

**Pizza sales**: $\mu = \$900/\text{day}$, $\sigma = \$300$, $n = 7$ days

$$SE(\bar{x}) = \frac{\$300}{\sqrt{7}} \approx \$113$$

A 7-day average will typically be within $\pm\$226$ (2 SEs) of the true mean. Sampling 28 days instead cuts the SE in half: $\frac{\$300}{\sqrt{28}} \approx \$57$.

### The Visual

As $n$ grows, the sampling distribution narrows around the true mean $\mu$:

![](images/week7/sampling_overlay.png){width="65%" fig-align="center"}

:::

# Part 3: The Central Limit Theorem

The theorem that makes all of statistical inference possible

## The Central Limit Theorem {.smaller}

**This is one of the most powerful theorems in all of statistics.**

> If repeated independent samples of size $N$ are drawn from **any population** (regardless of its shape) having mean $\mu$ and standard deviation $\sigma$, then — as $N$ becomes large — the sampling distribution of the sample mean approaches a Normal distribution:
>
> $$\bar{x} \;\sim\; N\!\left(\mu,\; \frac{\sigma}{\sqrt{N}}\right)$$

**Why is this remarkable?** The *population* can be skewed, bimodal, uniform — it doesn't matter. As long as $N$ is large enough, $\bar{x}$ is approximately Normal.

This is why the Normal distribution appears everywhere in statistics — and why the tools we build next all work.

## CLT: Interactive Demo {data-background-iframe="https://seeing-theory.brown.edu/probability-distributions/index.html#section3" data-background-interactive="true"}

## How Large a Sample? {.smaller}

The sampling distribution approaches normality faster for more symmetric populations:

- If the **population distribution is Normal**, the sampling distribution is Normal for *any* $n$
- The more **skewed** the population, the larger $n$ must be
- **In practice**: $n > 30$ is usually sufficient for most real-world distributions

::: {.fragment .fade-up}
**Why this matters for your research**: Most survey samples (GSS, IPUMS, etc.) have $n$ in the hundreds or thousands — well above the threshold where CLT guarantees apply. This is what lets us do inference from survey data without knowing the full population distribution.
:::

## Rolling Dice: A Worked Example {.smaller}

::: {.panel-tabset}

### Setup

**Question**: What is the probability that the mean of 35 dice rolls is less than 3?

**Step 1: Parameters of a single die roll**

A fair die has outcomes 1–6, each with probability 1/6:

- Mean: $\mu = \frac{1+2+3+4+5+6}{6} = 3.5$
- Variance: $\sigma^2 = \frac{(1-3.5)^2 + (2-3.5)^2 + \cdots + (6-3.5)^2}{6} = \frac{35}{12}$, so $\sigma = \sqrt{35/12}$

### Applying the CLT

**Step 2: Sampling distribution of $\bar{x}$** for $n = 35$ rolls

By the CLT — same $\frac{\sigma}{\sqrt{n}}$ formula as the Binomial SE from Week 6, now applied to any population:

$$SE(\bar{x}) = \frac{\sigma}{\sqrt{n}} = \frac{\sqrt{35/12}}{\sqrt{35}} = \frac{\sqrt{35}}{\sqrt{12} \cdot \sqrt{35}} = \frac{1}{\sqrt{12}} \approx 0.289$$

So $\bar{x} \sim N(3.5,\; 0.289)$

### Solution

**Step 3: Compute the probability in R**

```{r}
#| echo: true
#| eval: true

pnorm(3, mean = 3.5, sd = 1/sqrt(12))
```

The probability is approximately **4.2%** — rolling a mean below 3 across 35 dice is unusual but not impossible.

:::

## Key Takeaways {.smaller}

- **Populations** have parameters; **samples** have statistics — and they're different things
- A **sampling distribution** describes how a statistic varies across many hypothetical samples
- The **standard error** (SE) measures this variability — it shrinks as $\sqrt{n}$ grows
- The **Central Limit Theorem** guarantees that sample means are approximately Normal for large $n$, regardless of the population shape

> Sample → statistic → sampling distribution → inference. This is the chain that runs through the rest of the course.

## Why This Matters for the Rest of the Course {.smaller}

- **Next week (Week 8):** Confidence intervals use the SE and the Normal approximation to build a range of plausible parameter values
- **Week 9:** Hypothesis tests ask "how far is our statistic from what we'd expect?" — measured in standard errors
- **Weeks 11–13 (Regression):** Every regression coefficient comes with a standard error and a significance test — all built on exactly the CLT logic we developed today

> The CLT is not just this week's topic. It is the engine of all classical statistical inference. Once you understand it, you understand *why* every test we'll do actually works.

## Questions?

## Assignments {.smaller}

**Weekly Assignment #6**

- <span class="highlight">Due Thursday, March 12</span> by 11:59 PM
- Format: mix of word problems and R (similar to previous weeks)

**Annotated Bibliography**

- <span class="highlight">Due Thursday, March 19</span>
- Finding 10 relevant papers takes time — start early!
- Don't expect the first papers you find to be relevant to your project
- Happy to help during office hours

## In-class Lab #4 {.smaller}

1. Download `lab4.qmd` from bCourses under "assignments" > "Lab #4"
2. Place `lab4.qmd` in your `labs` folder
3. Use the `Explorer` button on the left to find and open `lab4.qmd`
4. Let's work through it together!
