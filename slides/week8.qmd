---
title: "Week 8"
subtitle: "Sociology 106: Quantitative Sociological Methods"
date: 03/10/2026
date-format: long
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: slides.css
    footer: '[GitHub course repo](https://www.kaseyzapatka.com/soc106)'
---

```{r}
#| echo: false
#| results: false

# code for lecture
# ---------

# load tidyverse
library(tidyverse)
library(here)

# load attain library
attain <- read_csv(here("data/attain.csv")) 
```

## Housekeeping {.smaller}

**Annotated Bibliography**: <span class="highlight">Due March 19</span>

Example of annotated bibliography posted on course site (in the Research Paper folder)

**General format**: 

- One paragraph summarizing the argument of the article
- One paragraph discussing how it relates to and/or helps to answer your research question

## Annotated Bibliography Example {.smaller}

![](images/week8/annotated_bib.png){width="100%" fig-align="center"}


## Agenda {.smaller}

- **Review last week's homework???**
- From sampling to inference
- Point estimators and their properties
- Confidence intervals
  - For proportions
  - For means
- Lab time to work on homework

## Last Week's Homework {.smaller}

[[**ADD IT IN LATER**]]

Some common issues and clarifications:

**Problem with small samples**: When using the sampling distribution of a proportion vs. binomial distribution

- Answers may differ slightly due to smaller sample size
- But these converge as $N$ becomes large

**Continuous distributions**: Probability of any exact value = 0

- But we can still compare likelihoods
- Remember: larger sample → sample mean closer to population mean

## Questions About Homework?

## Last Week: Recap {.smaller}

**What we learned:**

- **Sampling**: The collection of (approximately) random sets of observations from a larger population

- **Sampling distributions**: Probability distributions of sample statistics
  - We learned how to construct these given **known population parameters** and sample size

- **Central Limit Theorem**: For sufficiently large samples, sampling distributions are approximately normal

## This Week: Inference {.smaller}

Last week, we could construct sampling distributions because we knew the population parameters

**But in reality**: We almost never know the population parameters!

**This week**: We flip the problem around

- We use sample statistics to make inferences about **unknown population parameters**
- We quantify our uncertainty using **confidence intervals**

## Key Concepts {.smaller}

::::: columns
::: {.column width="50%"}

**Population**

- Parameter: unknown value we want to estimate
- Examples: $\mu$, $\pi$, $\sigma$

:::
::: {.column width="50%"}

**Sample**

- Statistic: observed value from our data
- Examples: $\bar{x}$, $\hat{p}$, $s$

:::
:::::

**The goal**: Use statistics to estimate parameters while accounting for uncertainty

## Estimators {.smaller}

An **estimator** is a rule (usually a mathematical formula) for making inferences about a population parameter using sample data

The value of an estimator is called an **estimate**

**Most summary statistics can be used as estimators:**

- Proportions
- Means
- Medians
- Variance and standard deviations
- Covariances and correlations

## Types of Estimators {.smaller}


There are two main kinds of estimators: **point estimators** and **interval estimators**

::: {.fragment .fade-up}
::: {.panel-tabset}

### Point Estimators

A **point estimator** of a population parameter is a single value that is used to predict the value of the parameter

**Examples**: All the summary statistics on the previous slide are point estimators

- Sample mean $\bar{x}$ estimates population mean $\mu$
- Sample proportion $\hat{p}$ estimates population proportion $\pi$

### Interval Estimators

An **interval estimator** of a population parameter is an interval that is predicted to contain the parameter

The **confidence level** we ascribe to the interval is the probability that it will contain the parameter

**Example**: A 95% confidence interval for the mean

:::
::: 

## Properties of Point Estimators {.smaller}

A point estimator has two important properties:

::: {.fragment .fade-up}
**1. Bias**: The difference between the expected value of the estimator in repeated samples and the population value of interest

- The estimator is **unbiased** if the difference is zero
- In a single sample, lack of bias = **accuracy**

::: 

::: {.fragment .fade-up}
**2. Efficiency**: The sampling variability of the estimator

- An estimator is **efficient** if its sampling variability is lowest among alternative estimators
- In a single sample, efficiency = **precision**

::: 

## Visualizing Bias and Efficiency {.smaller}

![Remember: bias relates to accuracy, efficiency relates to precision](images/week8/bias_vs_accuracy.png){width="100%" fig-align="center"}

## Confidence Intervals {.smaller}

To construct a **confidence interval** for a population parameter, we use the sampling distribution of the point estimator for that parameter

::: {.fragment .fade-up}
**Key components:**

1. The mean of the sampling distribution equals the point estimator itself
::: 

::: {.fragment .fade-up}
2. We use the **sample standard error** to approximate the standard deviation of the sampling distribution
   - Sometimes this is efficient; sometimes not
::: 

## Confidence Level and Margin of Error {.smaller}

::: {.fragment .fade-up}
**Confidence level**: The probability that a confidence interval contains the population parameter

- Most common: 95%
- Also used: 90%, 99%, and even 99.9%

::: 

::: {.fragment .fade-up}
**Margin of error**: A measure of how accurate the point estimate is likely to be in estimating a parameter

- Usually expressed in standard errors away from the point estimate
- Confidence interval = (point estimate ± margin of error)
::: 

::: {.fragment .fade-up}
- An example from election results: Candidate A is expected to receive `41% ± 3%` of the vote.
::: 


## Trade-offs in Confidence Intervals {.smaller}

**As confidence level increases** → margin of error increases

- To be more confident our interval contains the parameter, we need to expand our interval

**As sample size increases** → margin of error decreases

- With more data, we can be more certain the population parameter is close to the sample statistic
- So we can be as confident with a smaller interval

## Questions?

## Confidence Interval for a Proportion {.smaller}

The sample proportion $\hat{p}$ is an unbiased estimator of the population proportion $\pi$

The exact standard error is $\sqrt{\frac{\pi(1-\pi)}{n}}$, but we don't know $\pi$ in practice

**Therefore, we estimate the standard error as:**

$$SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

This standard error is also an *unbiased* estimator of the exact standard error

## Constructing the Confidence Interval {.smaller}

In large samples (at least 15 'successes' and 15 'failures'), the sampling distribution of a sample proportion is approximately normal

**Therefore**: We can use the normal distribution to calculate the margin of error

We express the margin of error in terms of the **z-score**: the number of standard deviations away from the mean in a standard normal distribution

## Formula for CI of a Proportion {.smaller}

The confidence interval for a population proportion $\pi$ in a large sample is:

$$\hat{p} \pm z^* \times SE(\hat{p})$$

where $z^*$ is the critical value from the standard normal distribution for the desired confidence level

::: {.fragment .fade-up}
**For a 95% confidence interval:**

$$\hat{p} \pm 1.96 \times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

critical value for a z-score for a 95% confidence interval is `~1.96` (we'll learn later)

::: 

## Worked Example {.smaller}

**Environmental Protection:**In 2000, the GSS asked: "Are you willing to pay much higher prices in order to protect the environment?"

- 518 said yes
- 636 said no

**Question**: What is the 95% confidence interval of the population proportion of adult Americans willing to do so at the time of the survey?

(Ignoring sample weights for now...)

## Worked Calculations {.smaller}

**Step 1**: Calculate sample proportion

::: {.fragment .fade-up}
$$\hat{p} = \frac{518}{518 + 636} = \frac{518}{1154} = 0.449$$
::: 

::: {.fragment .fade-up}
**Step 2**: Calculate standard error

$$SE(\hat{p}) = \sqrt{\frac{0.449(1-0.449)}{1154}} = \sqrt{\frac{0.449 \times 0.551}{1154}} = 0.0146$$
::: 

::: {.fragment .fade-up}
**Step 3**: Calculate 95% CI

$$0.449 \pm 1.96 \times 0.0146 = 0.449 \pm 0.029 = [0.420, 0.478]$$
::: 


## Example: Higher Confidence Level {.smaller}

Now let's say you wanted a higher confidence level--99% instead of 95%. You'd do the same calculations but use the critical value of 
$2.576$ instead of $1.96$

**For a 99% confidence interval** ($z^* = 2.576$):

$$0.449 \pm 2.576 \times 0.0146 = 0.449 \pm 0.038 = [0.411, 0.487]$$

::::: columns
::: {.column width="50%"}

**95% confidence interval**

$[0.420, 0.478]$
:::
::: {.column width="50%"}


**99% confidence interval**

$[0.411, 0.487]$
:::
:::::


> **Notice**: The 99% CI is wider than the 95% CI. We need a wider interval to be more confident it contains the true parameter


## Finding Critical Values in R {.smaller}

Many people used to look up z-scores for different confidence using in tables, but we can also use `R` to quickly do that as well. Here are some common CI intervals, with 95% being the most common. 

::: {.panel-tabset}

### 90% CI
```{r}
#| echo: true
#| eval: true
#| 
# For 90% confidence interval
qnorm(0.90)                           # one-tail
qnorm((1-0.90)/2, lower.tail = FALSE) # two-tail 
```

### 95% CI
```{r}
#| echo: true
#| eval: true

# For 95% confidence interval
qnorm(0.95)                           # one-tail
qnorm((1-0.95)/2, lower.tail = FALSE) # two-tail 
```

### 97% CI
```{r}
#| echo: true
#| eval: true

# For 97% confidence interval
qnorm(0.97)                           # one-tail
qnorm((1-0.97)/2, lower.tail = FALSE) # two-tail 
```


### 99% CI
```{r}
#| echo: true
#| eval: true

# For 99.5% confidence interval
qnorm(0.99)                           # one-tail
qnorm((1-0.99)/2, lower.tail = FALSE) # two-tail 

```

:::


## 1 vs 2-tailed tests{.smaller}

When we test for differences, we either test for them in one or both directions. 


::: {.panel-tabset}

### A one-tailed test 

A **one-tailed** test checks for a significant effect in one specific direction (e.g., greater than or less than)

### A two-tailed test 

A **two-tailed** test looks for any significant difference (greater than or less than) in either direction, making it more comprehensive but requiring more data to reach significance

:::

![](images/week8/1_vs_2tailed_test.png){width="40%" fig-align="center"}



## Questions?

## Confidence Interval for a Mean {.smaller}

The sample mean $\bar{x}$ is an unbiased estimator of the population mean $\mu$

The exact standard error is $\frac{\sigma}{\sqrt{n}}$—but we don't know $\sigma$ in practice, so **we estimate it using the standard error of the mean $s$:**

$$SE(\bar{x}) = \frac{s}{\sqrt{n}}$$

Let's look at each part in plain language:

::: {.panel-tabset}

### $\bar{x}$ – the sample mean

- $\bar{x}$ is the quantity we are trying to estimate (the population mean $\mu$)

### $s$ – sample standard deviation

- $s$ is the standard deviation of the sample means $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}$
- How spread out individual observations are
- More variability → more uncertainty in the mean

### $n$ – sample size

- $n$ is the size of each sample 
- Averaging more observations reduces randomness
- Larger $n$ → smaller standard error

:::

## The t-Distribution {.smaller}

**Problem**: In small samples, $s$ is a biased estimator of $\sigma$

::: {.fragment .fade-up}
**Solution**: To account for this increased error, we replace the z-score with a slightly larger score, the **t-score**
::: 

::: {.fragment .fade-up}
The t-score comes from the **t-distribution**, which is:

- Similar to the normal distribution (bell-shaped, symmetric around the mean)
- But with slightly larger tails—more spread out
- Shape depends on **degrees of freedom** (df): here, df = $n - 1$
:::

## Properties of the t-Distribution {.smaller}

**Key properties:**

- When df > 100, the t-distribution is basically the same as the normal distribution
- With smaller samples, t-distribution has fatter tails
- In small samples, we also need to assume the variable is approximately normally distributed in the population

![The t-distribution compared to the normal distribution](images/week8/t_distribution.png){width="80%" fig-align="center"}

## Formula for CI of a Mean {.smaller}

When the standard deviation of the population is unknown, the confidence interval for the population mean $\mu$ is:

$$\bar{x} \pm t^* \times \frac{s}{\sqrt{n}}$$

where $t^*$ is the critical value from the t-distribution with $n-1$ degrees of freedom

**To find the `t` critical value using R:**

::: {.panel-tabset}

### code

```{r}
#| echo: true
#| eval: false

qt((1 - conf_level)/2, df = n - 1, lower.tail = FALSE)
```

### worked example 

Let's assume we want a 95% confidence interval and our sample is 100. 

```{r}
#| echo: true

# confidence = 95%
# df = 100, 100 - 1
qt((1 - 0.95)/2, df = 99, lower.tail = FALSE) 
```

The $t^*$ critical value is therefore `~1.98`.

::: 


## Example: Heights {.smaller}

A study of 7 American adults from a simple random sample yields:

- Average height: 67.2 inches
- Standard deviation: 3.9 inches

Assuming heights are normally distributed, what is the 95% confidence interval for the average height of all American adults?

::: {.fragment .fade-up}
::: {.panel-tabset}

### Step 1


**Step 1**: Identify the values

- $\bar{x} = 67.2$
- $s = 3.9$
- $n = 7$
- $df = 7 - 1 = 6$

### Step 2 

**Step 2**: Find the critical t-value

```{r}
#| echo: true
#| eval: true

qt(0.975, df = 6)
```

### Step 3

**Step 3**: Calculate standard error

$$SE(\bar{x}) = \frac{3.9}{\sqrt{7}} = \frac{3.9}{2.646} = 1.474$$

### Step 4

**Step 4**: Calculate 95% CI

$$67.2 \pm 2.447 \times 1.474 = 67.2 \pm 3.6 = [63.6, 70.8]$$

**Interpretation**: We are 95% confident that the average height of all American adults is between 63.6 and 70.8 inches


::: 
::: 


## Comparing z and t {.smaller}

Notice that for this small sample ($n = 7$):

- t-critical value = 2.447
- z-critical value would be = 1.96

**The difference matters!** Using $z$ instead of $t$ would give us:

$$67.2 \pm 1.96 \times 1.474 = 67.2 \pm 2.9 = [64.3, 70.1]$$

This interval is too narrow and would not have the correct coverage

## When to Use z vs t {.smaller}

So, when do we use $z$ and when do we use $t$?

::: {.fragment .fade-up}
::::: columns
::: {.column width="50%"}

**Use the z-distribution when:**

- Estimating a population proportion (in large samples)
- Population standard deviation is actually known (rare!)
- Very large samples ($n > 100$) where $t ≈ z$

:::
::: {.column width="50%"}

**Use the t-distribution when:**

- Estimating a population mean
- Population standard deviation is unknown
- Especially important for small samples ($n < 30$)

:::
:::::
:::

## Summary {.smaller}

**This week we learned:**

1. How to use sample statistics as **estimators** of population parameters

2. The properties of good estimators: **bias** (accuracy) and **efficiency** (precision)

3. How to construct **confidence intervals** to quantify uncertainty:
   - For proportions (using z-distribution)
   - For means (using t-distribution)

4. How to interpret confidence intervals in context

## Homework {.smaller}


[[**UDPATE**]]

Using the dataset that you want to use in your final research project, calculate confidence intervals around:

1. The population mean of a proportion (based on the values of an indicator variable)

2. The population mean of a continuous variable

3. The population mean of a continuous variable for different values of an indicator variable

**Also**: Keep working on your annotated bibliography!

- <span class="highlight">Due March 19</span>
- Start early—finding relevant papers takes time