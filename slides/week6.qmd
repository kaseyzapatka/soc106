---
title: "Week 6"
subtitle: "Sociology 106: Quantitative Sociological Methods"
date: 02/24/2026
date-format: long
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: slides.css
    footer: '[GitHub course repo](https://www.kaseyzapatka.com/soc106)'
---

```{r}
#| echo: false
#| results: false

# code for lecture
# ---------

# load tidyverse
library(tidyverse)
library(here)
```

## Housekeeping {.smaller}


[[**ADD IT IN LATER**]]

## Agenda {.smaller}


- Random variables
- Probability models of distributions

**Discrete distributions:**

- **Binary distribution: **modeling whether an outcome occurs or not in one trial
- **Binomial distribution:** modeling how many occurrences we get of a binary outcome in multiple trials

**Continuous distributions:**

- **Normal distribution:** can be seen as an extension of the Bernoulli distribution to the continuous case

## Key takeaways {.smaller}

- **Random variables** → how we mathematically represent uncertainty
- **Probability distributions** → how likely each possible value is
- **Binary (Bernoulli) distribution** → the simplest possible case
- **Binomial distribution** → sums of binary variables
- **Normal distribution** → what binomial distributions approach when samples get large

> A **binary probability distribution** describes the **randomness** of a single yes/no outcome using one parameter, $π$; summing many such binary outcomes produces a **binomial distribution**, which in turn approaches a **normal distribution** as the number of trials grows large.

## Random Variables {.smaller}

Inferential statistics assume that the numerical values which a variable takes are the result of some **random phenomenon**

A **random variable** is: 

> a numerical measurement whose value is not known in advance because it depends on randomness

- **Rolling a die**: the outcome is random
- **The height of a random individual** in a population will depend on who we randomly select
- **Even your height** can be considered to be the result of randomness, once we condition on information we know about you (i.e., sex, age, ethnicity, parental height) that helps explain their height

## Random Variables: Definition {.smaller}

A **random variable** is a numerical measurement of a random phenomenon

**Notation:**

- **Capital letter $X$:** the random variable
- **Lowercase letter $x$:** a realized value


**Example**: Rolling a die

- $X$ = “outcome of a die roll”
- All the possible values for $x$ are the same as the values in $X$'s sample space S: `{1, 2, 3, 4, 5, 6}`
- but $x$=4 since “this roll came out as a 4”

## Probability Distributions {.smaller}

A **probability distribution** answers:

> If I repeated this random process many times, how often would I see each possible value?

**Remember**: Probability models consist of:

- The sample space of all possible outcomes
- A probability associated with each outcome

**Example**: For rolling a six-sided die: the probability distribution of the random variable $X$ is defined by:
$$P(X = x) = \frac{1}{6} \text{ for x} \in S:  \{1, 2, 3, 4, 5, 6\}$$

which fully describes the randomness of the die.

## Discrete Probability Distributions {.smaller}

In some cases, we can describe a probability distribution by listing all possible values a random variable can have, as well as the probabilities associated with each value

- This is generally the case with **categorical** (also known as **discrete**) probability distributions

However, just as we described a sample distribution in terms of summary statistics, we can also describe a probability distribution in terms of **parameters**

## Binary Probability Distributions {.smaller}

Let's strip things down to the simplest possible random variable: 

Perhaps the most basic *family* of discrete probability distributions is the **binary** (or **Bernoulli**) distribution, which models whether an outcome occurs or not in one trial

A **binary random variable** $X$ is equal to:

$$X \in \{0, 1\}$$


::::: columns
::: {.column width="50%"}

**Interpretation:**

- 1 if the outcome occurs
- 0 if the outcome does not occur

:::
::: {.column width="50%"}

**Examples:**

- Eviction occurred (1) or not (0)
- Voted (1) or didn’t vote (0)
- Policy passed (1) or failed (0)

:::
:::::

::: aside
This is why these are also called indicator variables.
:::

## Binary Probability Distributions {.smaller}

The probability distribution of a binary random variable:

$$P(X = 1) = \pi$$
$$P(X = 0) = 1 - \pi$$

**In plain language:** There are only two possible outcomes, so we only need one number to describe the entire distribution.

$$π= \text{probability the event occurs}$$

if $P(X = 1) = \pi$,  then automatically $P(X = 0) = 1 - \pi$ since probabilities sum to 1


## Binary Probability Distributions {.smaller}

**This is powerful** because this single parameter $\pi$:

- Completely defines the distribution
- Can vary from 0 to 1
- Represents the underlying data-generating process

Different values of $\pi$ → different worlds.

**Key insight**: Parameters are why we can refer to a **family** of binary probability distributions that share the same structure but can vary through having different parameter values

## Means of Random Variables {.smaller}

So why does $\pi$ matter?

- We can summarize information about the probability distribution using **parameters**
- The mean is the expected value over many repetitions.

**For binary $X$:**

$$\mu = 0 \cdot P(X=0) + 1 \cdot P(X=1) = \pi$$

**This is a HUGE result:** For a binary variable, the mean equals the probability of success.

**This is why:**

* Sample means of 0/1 variables are proportions
* Estimating probabilities = estimating means
* Linear probability models even exist

**Key insight:** This connects probability directly to statistics.

## Variance and Uncertainty {.smaller}

Recall that **variance**
 
>  measures how much randomness there is 

We calculate the variance by:

1. Summing (across all data points) each point's squared deviation from the mean
2. Dividing this sum by the number of observations in the data set, minus one

For a binary variable:

$$\text{Var}(X) = \pi(1-\pi)$$

::::: columns
::: {.column width="50%"}
**Key intuition:**

* Max variance at $\pi = 0.5$
* No variance at $\pi = 0$ or $\pi = 1$

:::
::: {.column width="50%"}

**So:**

* Perfect certainty → no randomness
* Most uncertainty → coin flip

:::
:::::


## Standard Deviation: Random Variables vs. Data {.smaller}

For **random variables**, we calculate the variance similarly, except we **weight** each value's squared deviation from the mean by the probability of that value occurring, instead of standardizing over the number of values

**Formulas:**

$$\text{Var}(X) = \sum P(X = x) \times (x - \mu)^2$$

$$\sigma(X) = \sqrt{\text{Var}(X)}$$

## Standard Deviation: Binary Random Variable {.smaller}

**Example**: Binary random variable X

$$\text{Var}(X) = P(X = 0) \times (0-\pi)^2 + P(X = 1) \times (1-\pi)^2$$

$$\text{Var}(X) = (1-\pi) \times \pi^2 + \pi \times (1-\pi)^2$$

$$\text{Var}(X) = (1-\pi) \times \pi \times (\pi + 1 - \pi)$$

$$\text{Var}(X) = (1-\pi) \times \pi$$

$$\rightarrow \sigma(X) = \sqrt{(1-\pi) \times \pi}$$

## Binary Probability Distributions: Summary {.smaller}

So, for a binary random variable X, where $P(X = 1) = \pi$ and $P(X = 0) = 1 - \pi$, we can calculate both:

- **Mean**: $\mu = \pi$
- **Standard deviation**: $\sigma = \sqrt{(1-\pi) \times \pi}$

Both are functions of the parameter $\pi$

**Notation**: We may write $X \sim \text{Binary}(\pi)$ (or $X \sim \text{Bernoulli}(\pi)$), which can be read as:

> "X is a random variable with a Binary (Bernoulli) probability distribution and mean $\pi$"

## Binary Probability Distribution: Visualization {.smaller}

We can graph the **probability density function**, which shows $P(X = x)$ across the sample space:

![](images/week6/bernouli_prob.png){width="100%,fig-align=\"center\""}


## Questions?

## Binomial Probability Distribution {.smaller}

The **binomial probability distribution** measures the distribution of the sum of n binary random variables, all with probability $\pi$

**Alternatively**: The distribution of n independent trials where 'success' occurs with probability $\pi$

**Notation**: If Y is a binomial random variable, we write $Y \sim B(n, \pi)$

**Question**: What are the possible values of Y?

## Binomial Probability Distribution: Possible Values {.smaller}

The binomial probability distribution measures the distribution of the sum of n binary random variables, all with probability $\pi$

**Alternatively**: The distribution of n **independent** trials where 'success' occurs with probability $\pi$

**Notation**: If Y is a binomial random variable, we write $Y \sim B(n, \pi)$

**Answer**: Y can equal any whole number between:

- 0 (all binary random variables equal 0) to
- n (all equal 1)

## Binomial Probability Distribution: Formula {.smaller}

**What are the probabilities of each value of Y?**

We can construct a general formula for $P(Y = k)$:

**Step 1**: Consider one sequence of n trials with k successes

- The k successes occur with probability $\pi^k$
- Similarly, the n-k failures occur with probability $(1-\pi)^{n-k}$
- Because each trial is independent, we multiply: $\pi^k \times (1-\pi)^{n-k}$

**Step 2**: Account for all possible sequences

- The k successes can occur anywhere among the n trials
- Number of ways = $\binom{n}{k} = \frac{n!}{k!(n-k)!}$, where $n! = 1 \times 2 \times 3 \times ... \times n$

## Binomial Probability Distribution: Formula {.smaller}

**Thus, we have:**

$$P(Y = k) = \binom{n}{k} \times \pi^k \times (1-\pi)^{n-k}$$

where $\binom{n}{k} = \frac{n!}{k!(n-k)!}$

## Binomial Probability Distribution: Example {.smaller}

**Example**: I shoot 5 three-pointers and I have a 30% chance of making each one. What is the probability I make exactly 2 three-pointers?

$Y \sim B(5, 0.3)$ (n = 5 trials with $\pi = 0.3$)

$$P(Y = 2) = \binom{5}{2} \times 0.3^2 \times (1-0.3)^{5-2}$$

$$= \frac{5!}{2! \times 3!} \times 0.3^2 \times 0.7^3$$

$$= \frac{1 \times 2 \times 3 \times 4 \times 5}{(1 \times 2) \times (1 \times 2 \times 3)} \times 0.3^2 \times 0.7^3$$

$$= 0.3087$$

**In R**: `dbinom(2, size = 5, prob = 0.3)`

## Binomial Distribution: Mean and Standard Deviation {.smaller}

We can do more math (not shown) to find that, for $Y \sim B(n, \pi)$:

**Mean:**

$$\mu = \pi \times n$$

The mean is equal to the probability of one trial succeeding, times the total number of trials

**Standard deviation:**

$$\sigma = \sqrt{\pi \times n \times (1-\pi)}$$

## PDF Shifts with $π$ {.smaller}

The **probability density function (pdf)** of a binomial random variable shifts with $\pi$...

![Shows how distribution shape changes with different values of π](images/week6/pdf_shifts.png){width="100%,fig-align=\"center\""}

## Binomial Distribution: Cumulative Probabilities {.smaller}

We can also answer questions like:

> "After n free throws made with probability π, what is the probability that I have made less than k free throws?"

In notation: If $Y \sim B(n, \pi)$, what is $P(Y \leq k)$?

**The challenge**: Centuries ago, if n and k were large, this would have been really difficult to answer!

**Example**: Say $Y \sim B(100, 0.5)$, and we want to know $P(Y \leq 65)$

This would require summing: $P(Y = 0) + P(Y = 1) + ... + P(Y = 65)$

**Today, in R**: 

```{r}
#| echo: true

pbinom(65, size = 100, prob = 0.5)

```


## Questions?


## Binomial Approaches Normal {.smaller}

The **key takeaway** if that as $n$ gets larger, the probability distribution function of a binomial random variable approaches a smooth curve:

<br> 


![](images/week6/pdf_to_normal.png){width="100%" fig-align="center"}

## From Bernoulli → Binomial {.smaller}

**The important statistical leap:** What if I repeat this binary process many times?

**Let:**

* Each trial be Bernoulli($\pi$)
* $Y$ = sum of $n$ binary variables

**Then:**

$$Y \sim \text{Binomial}(n, \pi)$$

**Interpretation:**

* $Y$ = number of successes
* Possible values: $0, 1, \ldots, n$

**This is literally just:** adding up 0s and 1s

## From Bernoulli → Binomial {.smaller}

As n gets larger, the probability distribution function of a binomial random variable approaches a smooth curve:

**(According to our textbook)**: The search for ways to approximate the binomial distribution at large values of $n$ led to the discovery of **normal distributions**

**Normal distributions** are a family of continuous probability distributions that are able to closely model the distribution of many natural phenomena

## Why Binomial Leads to Normal {.smaller}

As $n$ gets large:

- The binomial distribution becomes smooth
- It starts to look continuous
- It approaches a normal distribution

This is an early encounter with the **Central Limit Theorem**:

> Sums of many small random variables tend to look Normal.

**So historically:**

- People struggled to compute binomial probabilities
- Normal distributions were discovered as approximations
- Today, normals explain many natural phenomena

## Connecting It All Together {.smaller}

**One-sentence summary:**

A binary probability distribution describes the randomness of a single yes/no outcome using one parameter, $\pi$; summing many such binary outcomes produces a binomial distribution, which in turn approaches a normal distribution as the number of trials grows large.

**This connects:**

- Individual randomness (Bernoulli)
- Repeated processes (Binomial)
- Continuous phenomena (Normal)

## Normal Distributions {.smaller}

Normal distributions are defined by parameters for the **mean** ($\mu$) and the **standard deviation** ($\sigma$)

**The formula for the probability density function is:**

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

where:

- $\pi = 3.14159...$ (pi)
- $e = 2.71828...$ (the base of the natural logarithm)

**Notation**: We write $X \sim N(\mu, \sigma)$

That is, X is a normally distributed random variable with mean $\mu$ and standard deviation $\sigma$

## PDF of the Normal Distribution {.smaller}

![Classic bell curve showing symmetric distribution around mean μ](images/week6/pdf_of_normal.png){width="100%" fig-align="center"}

## Properties of the Normal Distribution {.smaller}

**Key properties:**

1. The normal distribution is **symmetric** around its mean

2. The **mean, median, and mode** of the normal distribution are the same

3. The **percentiles** of the distribution of a given random variable $X \sim N(\mu, \sigma)$ are known, such that:
   - X will be within 1σ of μ roughly **68%** of the time
   - X will be within 2σ of μ roughly **95%** of the time
   - X will be within 3σ of μ roughly **99.7%** of the time

## The 68-95-99.7 Rule {.smaller}

![Classic bell curve showing symmetric distribution around mean $μ$](images/week6/standard_normal.png){width="100%" fig-align="center"}

*Shows the percentage of data within 1, 2, and 3 standard deviations*

## The Standard Normal Distribution {.smaller}

Because all normal distributions share the same properties, we can **standardize** any normal distribution $N(\mu, \sigma)$ into the **standard normal distribution**, $N(0, 1)$

**How to standardize:**

We do so by:

1. First subtracting $\mu$
2. Then dividing by $\sigma$

$$Z = \frac{X - \mu}{\sigma}$$

## Z-Scores {.smaller}

For any given value of x in $X \sim N(\mu, \sigma)$, we can calculate the **z-score**, which is the number of standard deviations that a value x is away from $\mu$

**Formula:**

$$z = \frac{x - \mu}{\sigma}$$

**Interpretation:**

- If $x < \mu$, z is **negative**
- If $x > \mu$, z is **positive**

## Continuous Probability Distribution {.smaller}

**Question**: How do we assign probabilities to outcomes in an infinite sample space when we think the probability distribution is normal?

**Answer**: We compute probabilities for **intervals** under the normal density curve:

![Shows area under curve representing probability](images/week6/probabilities_on_normal.png){width="100%" fig-align="center"}


## Calculating Probabilities in R {.smaller}

**In R**: If $X \sim N(\mu, \sigma)$, we can:

**Calculate** $P(X \leq x)$ **as:**

```r
pnorm(x, mean = μ, sd = σ)
```

**Calculate the area under the normal density curve** of X in the interval $[x_1, x_2]$ as:

$$P(X \leq x_2) - P(X \leq x_1) = P(x_1 \leq X \leq x_2)$$

**In R:**

```r
pnorm(x2, μ, σ) - pnorm(x1, μ, σ)
```

## Finding Percentiles in R {.smaller}

We can also calculate what the value is at a given percentile q of X as:

```r
qnorm(q, mean = μ, sd = σ)
```

**Note**: q should be entered in decimal form (i.e., between 0 and 1, not between 0 and 100)

**Example**: To find the 95th percentile:

```r
qnorm(0.95, mean = μ, sd = σ)
```

## Questions?

## Weekly Assignment #5 {.smaller}

**Due**: Thursday, March 5 by 11:59 PM

**Format**: Similar to last week's weekly assignment: mostly a problem set

**Important**: You will use a little bit of R, so please submit your assignment (and your code!) on bCourses

**Rest of class**: Work on HW in groups

**Groups:**

- Group 1: *ADD*
- Group 2: *ADD*
- Group 3: *ADD*