---
title: "Week 6"
subtitle: "Sociology 106: Quantitative Sociological Methods"
date: 02/24/2026
date-format: long
format:
  revealjs:
    slide-number: true
    chalkboard:
      buttons: false
    preview-links: auto
    css: slides.css
    footer: '[GitHub course repo](https://www.kaseyzapatka.com/soc106)'
---

```{r}
#| echo: false
#| results: false

# code for lecture
# ---------

# load tidyverse
library(tidyverse)
library(here)
library(patchwork)
```

## Housekeeping {.smaller}

**HW 3:** Great job overall. Two main comments: 

- Make sure you are removing missing in your marginal frequency calculations (Q1: `filter(!is.na(degree), !is.na(marital)) |>`)
- Make sure to answer all the interpretation questions (e.g., Q4b asked to interpret the correlation coefficient)

## Where We Are in the Course {.smaller}

- **Last week (Week 5):** basics of probability theory
- **This week (Week 6):** probability models of distributions
- **Next week (Week 7):** sampling distributions
- **Then (Weeks 8-9):** confidence intervals and hypothesis tests

**Main idea:** Today is the bridge from probability language to inferential statistics.

## Agenda {.smaller}

- Random variables and probability distributions
- Discrete models:
  - **Bernoulli (Binary):** one yes/no outcome
  - **Binomial:** number of successes across repeated Bernoulli trials
- Continuous models:
  - **Normal:** a continuous model that often approximates Binomial outcomes when samples are large
- Implementation in R: `dbinom()`, `pbinom()`, `pnorm()`, `qnorm()`

## Our Running Example for Today {.smaller}

Imagine we're conducting a survey about voter turnout. Each person either voted (1) or didn't vote (0).

From prior research — like census records or previous surveys — we know that about **60% of eligible voters** in this population actually vote. So we'll set the probability of voting at $\pi = 0.6$.

Today we'll ask **three questions** about this scenario, and each one leads to a different probability distribution:

| Question | Distribution | Sample size |
|---|---|---|
| Did *this one person* vote? | **Bernoulli** | 1 person |
| Out of 10 people surveyed, how many voted? | **Binomial** | 10 people |
| Out of 100 people surveyed, roughly how many voted? | **Normal** (approximation) | 100 people |

## The Ladder: A Preview {.smaller}

These three distributions are connected — each is the same voting question at a different scale:

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 3.5

# Panel 1: Bernoulli (1 person)
p1 <- tibble(x = c("0", "1"), prob = c(0.4, 0.6)) |>
  ggplot(aes(x, prob)) +
  geom_col(fill = "#4E79A7", width = 0.5) +
  scale_y_continuous(limits = c(0, 0.75)) +
  labs(x = "X", y = "Probability",
       title = "1 person",
       subtitle = "Bernoulli(0.6)") +
  theme_minimal(base_size = 11)

# Panel 2: Binomial (10 people)
p2 <- tibble(k = 0:10, prob = dbinom(0:10, 10, 0.6)) |>
  ggplot(aes(k, prob)) +
  geom_col(fill = "#4E79A7", width = 0.6) +
  labs(x = "Y (number of voters)", y = "Probability",
       title = "10 people",
       subtitle = "Binomial(10, 0.6)") +
  theme_minimal(base_size = 11)

# Panel 3: Normal approximation (100 people)
mu <- 100 * 0.6; sigma <- sqrt(100 * 0.6 * 0.4)
p3 <- ggplot(tibble(x = c(40, 80)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 1, color = "#4E79A7") +
  labs(x = "Y (number of voters)", y = "Density",
       title = "100 people",
       subtitle = "Normal(60, 4.9)") +
  theme_minimal(base_size = 11)

p1 + p2 + p3
```

**Same event, different scale:** one person → a small sample → a large sample. We'll build up to this step by step.

## Random Variables and Distributions {.smaller}

Connect sample spaces and probability to data:

A **random variable** assigns a number to each outcome of a random process

  - For example, we code "voted" as 1 and "didn't vote" as 0

A **probability distribution** tells us how likely each value is

  - Discrete outcomes: a probability for each possible value (a **Probability Mass Function (PMF)**)
  - Continuous outcomes: probabilities come from areas under a curve (a **Probability Density Function (PDF)**)

## R Functions for Distributions {.smaller}

R uses a consistent naming pattern for working with probability distributions. Once you learn the prefixes, you can apply them to **any** distribution:

| Prefix | What it does | Question it answers | Example |
|-|---|---|---|
| **`d`** | **d**ensity / probability at a point | "Probability of **exactly** k successes?" | `dbinom(6, size = 10, prob = 0.6)` |
| **`p`** (Binomial) | cumulative count probability | "Probability of **at most** k successes in n trials?" | `pbinom(6, size = 10, prob = 0.6)` |
| **`p`** (Normal) | cumulative area probability | "Proportion of values **at or below** x?" | `pnorm(65, mean = 71, sd = 8)` |
| **`q`** | **q**uantile (inverse of `p`) | "What value has this **percentile**?" | `qnorm(0.95, mean = 60, sd = 4.9)` |


## R Functions for Distributions {.smaller}

The suffix tells you the distribution: `binom` for Binomial, `norm` for Normal.

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 3

par(mfrow = c(1, 3), mar = c(4, 3, 3, 1))

# d: probability at a point
barplot(dbinom(0:10, 10, 0.6), names.arg = 0:10, col = ifelse(0:10 == 6, "#E15759", "#4E79A7"),
        main = "d = exactly this value", xlab = "k", ylab = "", cex.main = 1.1, border = NA)

# p: cumulative probability
barplot(dbinom(0:10, 10, 0.6), names.arg = 0:10, col = ifelse(0:10 <= 6, "#4E79A7", "gray80"),
        main = "p = at most this value", xlab = "k", ylab = "", cex.main = 1.1, border = NA)

# q: quantile (inverse)
x <- seq(40, 80, length = 200)
plot(x, dnorm(x, 60, 4.9), type = "l", col = "#4E79A7", lwd = 2,
     main = "q = find the cutoff", xlab = "value", ylab = "", cex.main = 1.1)
cutoff <- qnorm(0.95, 60, 4.9)
polygon(c(x[x <= cutoff], cutoff), c(dnorm(x[x <= cutoff], 60, 4.9), 0),
        col = adjustcolor("#4E79A7", 0.3), border = NA)
abline(v = cutoff, col = "#E15759", lwd = 2, lty = 2)

par(mfrow = c(1, 1))
```

We'll use each of these as we go. Keep this pattern in mind!

# Did This Person Vote? The Bernoulli Model

The simplest probability distribution: one trial, two outcomes

## The Bernoulli Model {.smaller}

::::: columns
::: {.column width="42%"}

In our survey, each person's response is either 0 (didn't vote) or 1 (voted). Since we know from prior research that 60% of this population votes:

- P(voted) = P(X = 1) = **0.6**
- P(didn't vote) = P(X = 0) = **0.4**

**In general**, for any binary outcome with probability $\pi$:

$$P(X=1)=\pi, \quad P(X=0)=1-\pi$$

This is the **Bernoulli model**. The parameter $\pi$ is just the probability that the event occurs.

:::
::: {.column width="58%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 6
#| fig-height: 5.5

tibble(x = c(0, 1), prob = c(0.4, 0.6)) |>
  ggplot(aes(x = factor(x), y = prob)) +
  geom_col(fill = "#4E79A7", width = 0.5) +
  geom_text(aes(label = prob), vjust = -0.5, size = 7) +
  scale_y_continuous(limits = c(0, 0.75)) +
  labs(
    x = "X (0 = did not vote, 1 = voted)",
    y = "Probability",
    title = expression("Bernoulli PMF: " * pi * " = 0.6")
  ) +
  theme_minimal(base_size = 16)
```

:::
:::::

## Bernoulli Mean and Variance {.smaller}

Just as we computed summary statistics from data in Weeks 3–4, probability models have their own summary measures. These tell us what to **expect** before we collect data:

::: {.panel-tabset}

### Mean (expected value)

The long-run average of this random variable is just the probability itself:

$$E[X] = \pi = 0.6$$

If we survey many people one at a time, the average of all their 0/1 responses will be close to **0.6** — the proportion who voted.

This is why estimating a proportion *is* estimating a mean.

### Variance and SD

How much do individual outcomes vary around that average?

$$\text{Var}(X) = \pi(1-\pi) = 0.6 \times 0.4 = 0.24$$

$$\text{SD}(X) = \sqrt{\pi(1-\pi)} = \sqrt{0.24} \approx 0.49$$

- Variance is **largest** at $\pi = 0.5$ (maximum uncertainty — a coin flip)
- Variance is **smallest** near $\pi = 0$ or $\pi = 1$ (outcome almost certain)

:::

## Bernoulli: Sample Statistics vs. Model Parameters {.smaller}

| | From data<br>(what we compute) | From the model<br>(what we assume) | Voting example |
|-|---|---|---|
| **Center** | Sample proportion: $\hat{p}$ | $E[X]=\pi$ | $\hat{p}$ vs. $\pi = 0.6$ |
| **Spread** | Sample variance: $s^2$ | $\pi(1-\pi)$ | $s^2$ vs. $0.6 \times 0.4 = 0.24$ |

We compute $\hat{p}$ from our data (the fraction of respondents who said they voted). The model says the true probability is $\pi = 0.6$.

**The goal of statistical inference:** use the sample statistic ($\hat{p}$) to learn about the model parameter ($\pi$).

# How Many Voters in a Sample? The Binomial Model

Moving from one person to many: what happens when we add up Bernoulli outcomes?

## From Bernoulli to Binomial {.smaller}

::::: columns
::: {.column width="42%"}

Now suppose we survey **10 people**. Each person independently voted with probability 0.6.

**What's the probability that exactly 6 of the 10 voted?** Or 4? Or 8?

We're adding up 10 independent Bernoulli outcomes. The total, $Y$, follows a **Binomial distribution**:

$$Y \sim \text{Binomial}(n=10, \; \pi=0.6)$$

The chart shows the probability of **every possible outcome** from 0 to 10 voters. The tallest bar (k = 6) is the single most likely result.

:::
::: {.column width="58%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 6
#| fig-height: 5.5

tibble(k = 0:10, prob = dbinom(0:10, size = 10, prob = 0.6)) |>
  mutate(highlight = ifelse(k == 6, "k = 6", "other")) |>
  ggplot(aes(x = factor(k), y = prob, fill = highlight)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = round(prob, 3)), vjust = -0.3, size = 3.5) +
  scale_fill_manual(values = c("k = 6" = "#E15759", "other" = "#4E79A7")) +
  scale_y_continuous(limits = c(0, 0.30)) +
  labs(
    x = "k (number of voters out of 10)",
    y = "P(Y = k)",
    title = "Binomial PMF: B(10, 0.6)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```

:::
:::::

## Binomial Assumptions {.smaller}

The Binomial model requires four things to be true. Here's what each means for our voting example:

1. **Fixed number of trials:** we survey exactly $n = 10$ people
2. **Binary outcomes:** each person either voted or didn't
3. **Independence:** one person's vote doesn't affect another's
4. **Same probability:** each person has the same $\pi = 0.6$ chance of having voted

In real social data, **independence is the assumption most likely to fail** — people in the same household, neighborhood, or social network influence each other.

## Computing a Binomial Probability {.smaller}

**Question:** What's the probability that exactly 6 of our 10 people voted?

We need two ingredients:

- The probability of any **one specific sequence** with 6 voters and 4 non-voters: $0.6^6 \times 0.4^4$
- The **number of ways** to arrange 6 voters among 10 people: $\binom{10}{6} = 210$

Putting it together:

$$P(Y=6) = \underbrace{210}_{\text{arrangements}} \times \underbrace{0.6^6 \times 0.4^4}_{\text{probability of each}} = 0.2508$$

In R, `dbinom()` does this calculation for us:

```r
dbinom(6, size = 10, prob = 0.6)
#        ^       ^          ^
#        k       n          π
# "probability of exactly 6 successes in 10 trials with π = 0.6"
```

## Binomial Mean and Standard Deviation {.smaller}

For our sample of 10 people with $\pi = 0.6$:

**Expected number of voters:**

$$E[Y]=n\pi = 10 \times 0.6 = 6 \text{ voters}$$

**Standard deviation** (how much the count varies from sample to sample):

$$\text{SD}(Y)=\sqrt{n\pi(1-\pi)} = \sqrt{10 \times 0.6 \times 0.4} \approx 1.55$$

We'd expect about **6 voters, give or take about 1.5**. In most samples, we'd see somewhere between 4 and 8 voters.

## Cumulative Probabilities: "At Most" Questions {.smaller}

Often we want to know the probability of getting **at most** some number of successes. This is a **cumulative probability**: $P(Y \le k)$.

**Example:** What's the probability that at most 4 of our 10 people voted?

```{r}
#| echo: true

# P(Y ≤ 4) for Y ~ Binomial(10, 0.6)
pbinom(4, size = 10, prob = 0.6)
```

`dbinom()` gives the probability at **exactly one value** ($P(Y=k)$), while `pbinom()` **adds up all the bars** from 0 to $k$ to give $P(Y \le k)$.

**What about "at least" questions?** Use the complement: $P(Y \ge k) = 1 - P(Y \le k-1)$

```{r}
#| echo: true

# P(at least 7 voted) = 1 - P(at most 6 voted)
1 - pbinom(6, size = 10, prob = 0.6)
```

## Practice: Binomial Probabilities {.smaller .scrollable}

Let's some scenarios that are similar to HW questions.

::: {.panel-tabset}

### "At least" (Q1-style)

**Lisa makes free throws 75% of the time and shoots 8 free throws at practice.** What is the probability she makes **at least 7**?

::: {.fragment}
::::: columns
::: {.column width="50%"}

$P(X \ge 7)$ means $X = 7$ **or** $X = 8$. Use the complement:

$$P(X \ge 7) = 1 - P(X \le 6)$$

```{r}
#| echo: true
1 - pbinom(6, size = 8, prob = 0.75)
```

About a **37%** chance she makes at least 7 of 8.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
tibble(k = 0:8, prob = dbinom(0:8, size = 8, prob = 0.75)) |>
  mutate(highlight = ifelse(k >= 7, "at least 7", "other")) |>
  ggplot(aes(x = factor(k), y = prob, fill = highlight)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = round(prob, 3)), vjust = -0.3, size = 3) +
  scale_fill_manual(values = c("at least 7" = "#E15759", "other" = "#4E79A7")) +
  scale_y_continuous(limits = c(0, 0.35)) +
  labs(x = "k (free throws made)", y = "P(X = k)",
       title = "Binomial(8, 0.75): P(X \u2265 7)") +
  theme_minimal(base_size = 12) + theme(legend.position = "none")
```

:::
:::::
:::

### "Between" (Q1-style)

**Same scenario: Lisa, n = 8, π = 0.75.** What is the probability she makes **between 4 and 6** free throws?

::: {.fragment}
::::: columns
::: {.column width="50%"}

$P(4 \le X \le 6)$ — add up bars from 4 to 6. Equivalently:

$$P(4 \le X \le 6) = P(X \le 6) - P(X \le 3)$$

```{r}
#| echo: true
pbinom(6, size = 8, prob = 0.75) -
  pbinom(3, size = 8, prob = 0.75)
```

About a **61%** chance she makes between 4 and 6.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
tibble(k = 0:8, prob = dbinom(0:8, size = 8, prob = 0.75)) |>
  mutate(highlight = ifelse(k >= 4 & k <= 6, "4 to 6", "other")) |>
  ggplot(aes(x = factor(k), y = prob, fill = highlight)) +
  geom_col(width = 0.6) +
  geom_text(aes(label = round(prob, 3)), vjust = -0.3, size = 3) +
  scale_fill_manual(values = c("4 to 6" = "#E15759", "other" = "#4E79A7")) +
  scale_y_continuous(limits = c(0, 0.35)) +
  labs(x = "k (free throws made)", y = "P(X = k)",
       title = "Binomial(8, 0.75): P(4 \u2264 X \u2264 6)") +
  theme_minimal(base_size = 12) + theme(legend.position = "none")
```

:::
:::::
:::

### Passing threshold (Q2-style)

**Same scenario: Javier, n = 50, π = 0.80. He needs at least 35 correct (70%) to pass.** What's the probability he passes?

::: {.fragment}
::::: columns
::: {.column width="50%"}

70% of 50 = **35**. "Pass" means $X \ge 35$:

$$P(X \ge 35) = 1 - P(X \le 34)$$

```{r}
#| echo: true
1 - pbinom(34, size = 50, prob = 0.80)
```

About a **97%** chance Javier passes.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
tibble(k = 30:50, prob = dbinom(30:50, size = 50, prob = 0.8)) |>
  mutate(highlight = ifelse(k >= 35, "passing", "failing")) |>
  ggplot(aes(x = factor(k), y = prob, fill = highlight)) +
  geom_col(width = 0.7) +
  scale_fill_manual(values = c("passing" = "#4E79A7", "failing" = "#E15759")) +
  labs(x = "k (questions correct)", y = "P(Y = k)",
       title = "Binomial(50, 0.80): P(Y \u2265 35)") +
  theme_minimal(base_size = 11) +
  theme(legend.position = "none", axis.text.x = element_text(size = 8))
```

:::
:::::
:::

:::

## Binomial: Sample Statistics vs. Model Parameters {.smaller}

| | From data<br>(what we compute) | From the model<br>(what we assume) | Voting example ($n = 10$) |
|-|---|---|---|
| **Center** | Sample count of voters | $E[Y] = n\pi$ | observed count vs. $10 \times 0.6 = 6$ |
| **Spread** | Sample SD of counts | $\text{SD}(Y) = \sqrt{n\pi(1-\pi)}$ | sample SD vs. $\approx 1.55$ |

If we repeated our survey of 10 people many times, the average number of voters across all surveys would be close to **6**, and the counts would typically range from about **4 to 8**.

# What About Larger Samples? The Normal Approximation

As sample size grows, the Binomial starts to look like a smooth bell curve

## Why Binomial Starts Looking Normal {.smaller}

What if we surveyed **100 people** instead of 10? As $n$ grows, the Binomial distribution becomes **smoother** and starts to resemble a bell-shaped curve:

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-height: 4.5

set.seed(106)

tibble(
  n = c(1, 10, 50, 100)
) |>
  mutate(
    n_label = paste0("n = ", n),
    n_label = factor(n_label, levels = paste0("n = ", c(1, 10, 50, 100))),
    draws = map(n, ~ rbinom(5000, size = .x, prob = 0.6))
  ) |>
  unnest(draws) |>
  ggplot(aes(x = draws, y = after_stat(density))) +
  geom_histogram(binwidth = 1, color = "white", fill = "#4E79A7") +
  facet_wrap(~ n_label, scales = "free") +
  labs(
    x = "Number of voters",
    y = "Density",
    title = "Binomial(n, 0.6): from one person to 100"
  ) +
  theme_minimal(base_size = 14)
```

This is an early step toward **central limit thinking**: sums of many random components often look approximately Normal.

## The Normal Distribution {.smaller}

::::: columns
::: {.column width="45%"}

For $n = 100$ voters with $\pi = 0.6$, the Binomial has:

- Mean: $100 \times 0.6 = 60$ voters
- SD: $\sqrt{100 \times 0.6 \times 0.4} \approx 4.9$ voters

**What does SD = 4.9 mean?** In most surveys of 100 people, the number of voters will be **within about 5 of 60** — typically between 55 and 65.

Instead of computing exact Binomial probabilities across 101 values, we approximate with a **Normal distribution**: $N(60, 4.9)$.

:::
::: {.column width="55%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 5

mu <- 100 * 0.6
sigma <- sqrt(100 * 0.6 * 0.4)

ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 1, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.3) +
  geom_vline(xintercept = mu, linetype = "dashed", color = "gray40") +
  annotate("text", x = mu, y = -0.003, label = "60", size = 5) +
  annotate("segment", x = mu, xend = mu + sigma, y = 0.06, yend = 0.06,
           arrow = arrow(length = unit(0.15, "cm"), ends = "both"), color = "#E15759") +
  annotate("text", x = mu + sigma/2, y = 0.065, label = expression(SD %~~% 4.9),
           size = 4, color = "#E15759") +
  labs(
    x = "Number of voters (Y)",
    y = "Density",
    title = "Normal(60, 4.9) - approximation to Binomial(100, 0.6)"
  ) +
  theme_minimal(base_size = 14)
```

:::
:::::

For continuous models, probabilities come from **areas under the curve**.

## Key Properties of Normal Distributions {.smaller}

::::: columns
::: {.column width="50%"}

1. Symmetric around the mean
2. Mean, median, and mode coincide
3. **68-95-99.7 rule:**
   - within 1 SD ($\pm 4.9$): about 68% of samples → **55 to 65 voters**
   - within 2 SD ($\pm 9.8$): about 95% of samples → **50 to 70 voters**
   - within 3 SD ($\pm 14.7$): about 99.7% of samples → **45 to 75 voters**

This rule gives us a quick way to judge whether an observed result is "typical" or "unusual."

:::
::: {.column width="50%"}

![](images/week6/standard_normal.png){width="100%" fig-align="center"}

:::
:::::

## Standardization and Z-Scores {.smaller}

::::: columns
::: {.column width="45%"}

A **z-score** tells you how many standard deviations a value is from the mean:

$$z = \frac{\text{observed} - \text{mean}}{\text{SD}}$$

**Example:** We observed **70 voters** out of 100. How unusual is that?

$$z = \frac{70 - 60}{4.9} \approx 2.04$$

That's about **2 SDs above the mean**. By the 68-95-99.7 rule, only about **2.5%** of samples would show 70+ voters.

:::
::: {.column width="55%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 5

mu <- 60; sigma <- 4.9

ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 1, color = "#4E79A7") +
  geom_vline(xintercept = 60, linetype = "dashed", color = "gray50") +
  geom_vline(xintercept = 70, linetype = "solid", color = "#E15759", linewidth = 1) +
  annotate("text", x = 60, y = 0.085, label = "Expected: 60", size = 4, hjust = 1.1) +
  annotate("text", x = 70, y = 0.085, label = "Observed: 70", size = 4,
           color = "#E15759", hjust = -0.1) +
  annotate("text", x = 65, y = 0.05, label = expression(z %~~% 2.04), size = 5, fontface = "bold") +
  labs(
    x = "Number of voters",
    y = "Density",
    title = "How unusual is 70 voters out of 100?"
  ) +
  theme_minimal(base_size = 14)
```

:::
:::::

## Computing Normal Probabilities in R {.smaller}

::: {.panel-tabset}

### Between two values

What's the probability of observing **between 55 and 65** voters?

::::: columns
::: {.column width="45%"}

```{r}
#| echo: true
pnorm(65, mean = 60, sd = 4.9) -
  pnorm(55, mean = 60, sd = 4.9)
```

**In general:** $P(a \le Y \le b) = P(Y \le b) - P(Y \le a)$

:::
::: {.column width="55%"}

```{r}
#| echo: false
#| fig-height: 3.5
mu <- 60; sigma <- 4.9
ggplot(tibble(x = c(40, 80)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), geom = "area", xlim = c(55, 65), fill = "#4E79A7", alpha = 0.4) +
  annotate("text", x = 60, y = 0.04, label = "69.2%", size = 5, fontface = "bold") +
  labs(x = "Number of voters", y = NULL, title = expression(P(55 <= Y) <= 65)) +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::

### At most

What's the probability of observing **at most 50** voters?

::::: columns
::: {.column width="45%"}

```{r}
#| echo: true
pnorm(50, mean = 60, sd = 4.9)
```

This is $P(Y \le 50)$ — the area to the **left** of 50.

:::
::: {.column width="55%"}

```{r}
#| echo: false
#| fig-height: 3.5
ggplot(tibble(x = c(40, 80)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), geom = "area", xlim = c(40, 50), fill = "#4E79A7", alpha = 0.4) +
  annotate("text", x = 47, y = 0.02, label = "2.1%", size = 5, fontface = "bold") +
  labs(x = "Number of voters", y = NULL, title = expression(P(Y <= 50))) +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::

### More than

What's the probability of observing **more than 70** voters?

::::: columns
::: {.column width="45%"}

```{r}
#| echo: true
1 - pnorm(70, mean = 60, sd = 4.9)
```

We use $1 - P(Y \le 70)$ because `pnorm()` gives the area to the **left**.

:::
::: {.column width="55%"}

```{r}
#| echo: false
#| fig-height: 3.5
ggplot(tibble(x = c(40, 80)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), geom = "area", xlim = c(70, 80), fill = "#E15759", alpha = 0.4) +
  annotate("text", x = 73, y = 0.015, label = "2.1%", size = 5, fontface = "bold", color = "#E15759") +
  labs(x = "Number of voters", y = NULL, title = "P(Y > 70)") +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::

### Percentile cutoff

Below what count do **95%** of samples fall?

::::: columns
::: {.column width="45%"}

```{r}
#| echo: true
qnorm(0.95, mean = 60, sd = 4.9)
```

`qnorm()` is the **inverse** of `pnorm()` — it goes from probability to value.

:::
::: {.column width="55%"}

```{r}
#| echo: false
#| fig-height: 3.5
cutoff <- qnorm(0.95, mean = mu, sd = sigma)
ggplot(tibble(x = c(40, 80)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), geom = "area", xlim = c(40, cutoff), fill = "#4E79A7", alpha = 0.3) +
  geom_vline(xintercept = cutoff, linetype = "dashed", color = "#E15759", linewidth = 1) +
  annotate("text", x = cutoff + 1, y = 0.05, label = round(cutoff, 1), size = 5, color = "#E15759", hjust = 0) +
  annotate("text", x = 60, y = 0.04, label = "95%", size = 5, fontface = "bold") +
  labs(x = "Number of voters", y = NULL, title = "95th percentile") +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::

:::

## Practice: Z-Scores and Normal Probabilities {.smaller .scrollable}

These examples are similar to what you'll see on HW #5. Let's work through them together.

::: {.panel-tabset}

### Z-score by hand

**Andre's height is 75 inches.** Heights in his population are Normal with mean 69 inches and SD 3 inches. Calculate his z-score.

::: {.fragment}
::::: columns
::: {.column width="50%"}

$$z = \frac{75 - 69}{3} = 2.0$$

Andre is **2 standard deviations above the mean**. By the 68-95-99.7 rule, he is taller than about 97.5% of the population — only **2.5%** of people are taller.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
mu <- 69; sigma <- 3
ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", xlim = c(75, mu + 4*sigma),
                fill = "#E15759", alpha = 0.5) +
  geom_vline(xintercept = 75, linetype = "dashed", color = "#E15759", linewidth = 1) +
  annotate("text", x = 78, y = 0.055, label = "2.5%", size = 5,
           color = "#E15759", fontface = "bold") +
  annotate("text", x = 74.5, y = 0.115, label = "75 in\n(z = 2.0)", size = 3.5,
           color = "#E15759", hjust = 1) +
  labs(x = "Height (inches)", y = NULL, title = "Normal(69, 3)") +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::
:::

### pnorm(): area to the left

**Vehicle speeds on I-5** are approximately Normal with mean 71 mph and SD 8 mph. The speed limit is 65 mph. What proportion of vehicles are at or below the speed limit?

::: {.fragment}
::::: columns
::: {.column width="50%"}

```{r}
#| echo: true
pnorm(65, mean = 71, sd = 8)
```

Only about **23%** of vehicles are going at or below 65 mph.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
mu <- 71; sigma <- 8
ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", xlim = c(mu - 4*sigma, 65),
                fill = "#4E79A7", alpha = 0.5) +
  geom_vline(xintercept = 65, linetype = "dashed", color = "#E15759", linewidth = 1) +
  annotate("text", x = 52, y = 0.02, label = "23%", size = 5,
           color = "#4E79A7", fontface = "bold") +
  annotate("text", x = 66, y = 0.043, label = "65 mph", size = 3.5,
           color = "#E15759", hjust = 0) +
  labs(x = "Speed (mph)", y = NULL, title = expression(P(Y <= 65) == 0.227)) +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::
:::

### 1 - pnorm(): area to the right

**SAT scores** are approximately Normal with mean 1026 and SD 209. The NCAA requires a score above 820 to compete. What proportion of students qualify?

::: {.fragment}
::::: columns
::: {.column width="50%"}

```{r}
#| echo: true
1 - pnorm(820, mean = 1026, sd = 209)
```

About **84%** of students score above 820.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
mu <- 1026; sigma <- 209
ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", xlim = c(820, mu + 4*sigma),
                fill = "#4E79A7", alpha = 0.5) +
  geom_vline(xintercept = 820, linetype = "dashed", color = "#E15759", linewidth = 1) +
  annotate("text", x = 1150, y = 0.001, label = "84%", size = 5,
           color = "#4E79A7", fontface = "bold") +
  annotate("text", x = 850, y = 0.0017, label = "820", size = 3.5,
           color = "#E15759", hjust = 0) +
  labs(x = "SAT Score", y = NULL, title = "P(Y > 820) = 0.838") +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::
:::

### pnorm() - pnorm(): between values

Using the same SAT distribution, what proportion score **between 720 and 820** (partial qualifiers)?

::: {.fragment}
::::: columns
::: {.column width="50%"}

```{r}
#| echo: true
pnorm(820, mean = 1026, sd = 209) - pnorm(720, mean = 1026, sd = 209)
```

About **9%** of students fall in this range.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
mu <- 1026; sigma <- 209
ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.2) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", xlim = c(720, 820),
                fill = "#E15759", alpha = 0.5) +
  geom_vline(xintercept = c(720, 820), linetype = "dashed",
             color = "#E15759", linewidth = 0.8) +
  annotate("text", x = 770, y = 0.001, label = "9%", size = 5,
           color = "#E15759", fontface = "bold") +
  annotate("text", x = 715, y = 0.0017, label = "720", size = 3.5,
           color = "#E15759", hjust = 1) +
  annotate("text", x = 825, y = 0.0017, label = "820", size = 3.5,
           color = "#E15759", hjust = 0) +
  labs(x = "SAT Score", y = NULL, title = "P(720 \u2264 Y \u2264 820) = 0.091") +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::
:::

### qnorm(): find the cutoff

**Vehicle speeds** again (mean 71, SD 8). A new speed limit will be set so that 10% of vehicles exceed it. What should the new limit be?

::: {.fragment}
::::: columns
::: {.column width="50%"}

```{r}
#| echo: true
qnorm(0.90, mean = 71, sd = 8)
```

The new speed limit should be about **81 mph** — 90% of vehicles travel at or below this speed.

:::
::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3.5
mu <- 71; sigma <- 8
cutoff <- qnorm(0.90, mean = mu, sd = sigma)
ggplot(tibble(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 0.8, color = "#4E79A7") +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", xlim = c(mu - 4*sigma, cutoff),
                fill = "#4E79A7", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", xlim = c(cutoff, mu + 4*sigma),
                fill = "#E15759", alpha = 0.4) +
  geom_vline(xintercept = cutoff, linetype = "dashed",
             color = "#E15759", linewidth = 1) +
  annotate("text", x = 65, y = 0.025, label = "90%", size = 5,
           color = "#4E79A7", fontface = "bold") +
  annotate("text", x = 88, y = 0.012, label = "10%", size = 5,
           color = "#E15759", fontface = "bold") +
  annotate("text", x = cutoff + 0.5, y = 0.038,
           label = paste0(round(cutoff, 1), " mph"),
           size = 3.5, color = "#E15759", hjust = 0) +
  labs(x = "Speed (mph)", y = NULL, title = "90th percentile cutoff") +
  theme_minimal(base_size = 12) + theme(axis.text.y = element_blank())
```

:::
:::::
:::

:::

## R Translation Guide {.smaller}

| Statistical question | R function |
|---|---|
| Exact Binomial probability $P(Y=k)$ | `dbinom(k, size = n, prob = pi)` |
| Cumulative Binomial probability $P(Y\le k)$ | `pbinom(k, size = n, prob = pi)` |
| Normal cumulative probability $P(Y\le x)$ | `pnorm(x, mean = mu, sd = sigma)` |
| Normal percentile / quantile | `qnorm(q, mean = mu, sd = sigma)` |

## Normal: Sample Statistics vs. Model Parameters {.smaller}

| | From data (what we compute) | From the model (what we assume) | Voting example ($n = 100$) |
|---|---|---|---|
| **Center** | Sample count of voters | $\mu = n\pi$ | observed count vs. $100 \times 0.6 = 60$ |
| **Spread** | Sample SD | $\sigma = \sqrt{n\pi(1-\pi)}$ | sample SD vs. $\approx 4.9$ |

If we repeated our survey of 100 people many times, the counts would center around **60** and mostly fall **between about 50 and 70** (within 2 SDs).

## Connecting It All Together {.smaller}

> A Bernoulli model describes one yes/no outcome with parameter $\pi$; summing many Bernoulli outcomes gives a Binomial model, and as trials grow large, that Binomial model is often well approximated by a Normal distribution.

This connects:

- Individual randomness (Bernoulli) — **1 person**
- Repeated processes (Binomial) — **10 people**
- Continuous approximation and inference tools (Normal) — **100 people**

## The Ladder, Visually {.smaller}

One trial → many trials → smooth approximation, all with $\pi = 0.6$:

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 4

# Panel 1: Bernoulli (1 person)
p1 <- tibble(x = c("0", "1"), prob = c(0.4, 0.6)) |>
  ggplot(aes(x, prob)) +
  geom_col(fill = "#4E79A7", width = 0.5) +
  scale_y_continuous(limits = c(0, 0.75)) +
  labs(x = "X", y = "Probability",
       title = "1 person",
       subtitle = "Bernoulli(0.6)") +
  theme_minimal(base_size = 12)

# Panel 2: Binomial (10 people)
p2 <- tibble(k = 0:10, prob = dbinom(0:10, 10, 0.6)) |>
  ggplot(aes(k, prob)) +
  geom_col(fill = "#4E79A7", width = 0.6) +
  labs(x = "Y (number of voters)", y = "Probability",
       title = "10 people",
       subtitle = "Binomial(10, 0.6)") +
  theme_minimal(base_size = 12)

# Panel 3: Normal approximation (100 people)
mu <- 60; sigma <- sqrt(100 * 0.6 * 0.4)
p3 <- ggplot(tibble(x = c(40, 80)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                geom = "area", fill = "#4E79A7", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 1, color = "#4E79A7") +
  labs(x = "Y (number of voters)", y = "Density",
       title = "100 people",
       subtitle = "Normal(60, 4.9)") +
  theme_minimal(base_size = 12)

p1 + p2 + p3
```

**Same event, three levels of analysis.** This is the logic that powers the rest of the course.

## Key Takeaways {.smaller}

- **Random variables** represent uncertainty numerically
- **Probability distributions** assign probabilities to possible values
- **Bernoulli** models one binary trial → mean is $\pi$
- **Binomial** models the sum of many Bernoulli trials → mean is $n\pi$
- **Normal** approximates the Binomial when $n$ is large
- **Statistical inference** uses sample statistics to learn about model parameters

> One trial (Bernoulli) → many trials (Binomial) → smooth large-sample pattern (Normal)

## Why This Matters for the Rest of the Semester {.smaller}

- **Next week (Week 7):** sampling distributions build directly on this Binomial-to-Normal logic
- **Weeks 8-9:** confidence intervals and hypothesis tests rely on Normal-based approximations
- **Later regression units:** model interpretation and uncertainty quantification use the same distributional thinking

Today is not isolated content. It is core infrastructure for the rest of SOC 106.

## Questions?

## Weekly Assignment #5 {.smaller}

**Due**: Thursday, March 5 by 11:59 PM

**Format**: Similar to last week's weekly assignment: mostly a problem set

**Important**: You will use a little bit of R, so please submit your assignment (and your code!) on bCourses


## Paper Proposal (5%) {.smaller}

A **two-page double-spaced proposal** for your final paper is due on bCourses by **Thursday, February 26 at 11:59 PM**. Here's [an example](https://www.kaseyzapatka.com/soc106/assignments/paper/01_proposal.html). 

**Your proposal should include:**

1. **Research question** - What are you trying to answer?
2. **Why it matters** - Why should we care about this question?
3. **Hypotheses** - What do you think the answer is, and why?
4. **Data source** - What dataset will you use?
5. **Key variables** - Identify your independent and dependent variables

*Note: You do not need to discuss statistical techniques at this point.*

## In-class Lab #3 {.smaller}

Let's practice some of what we learned today:

1. Download `lab3.qmd` from bCourse under "assignments" > "Lab #3"
2. Place `lab3.qmd` in your `labs` folder. 
3. Use the `Explorer` button on the left to find and open `lab3.qmd`
4. Let's work through it together!
