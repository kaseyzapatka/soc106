---
title: "Week 4"
subtitle: "Sociology 106: Quantitative Sociological Methods"
date: 02/10/2026
date-format: long
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: slides.css
    footer: '[GitHub course repo](https://www.kaseyzapatka.com/soc106)'
---

```{r}

```
```{r}
#| echo: false
#| results: false

# code for lecture
# ---------

# load tidyverse
library(tidyverse)
library(here)

# load attain library
attain <- read_csv(here("data/attain.csv")) 

```

## Housekeeping {.smaller}

[[**ADD IT IN LATER**]]

## Questions?

## Agenda {.smaller}

**Summarizing distributions:**

- Relative and marginal frequencies
- Measures of the central tendency
  - Mean, median, and mode
- Measures of dispersion
  - Percentiles and standard deviation / variance
- Association between two variables
  - Comparing proportions: difference in proportions, relative risk ratio
  - Comparing distributions: covariance and correlation

## Relative frequencies {.smaller}

**Relative frequency** (aka proportion): the frequency of observations for a given variable, divided by the total number of observations.

- This is similar to a probability, though statisticians generally talk about probabilities existing "in the world" and relative frequencies existing "within the data"

**Notation:**

- $p_{X=i} = \frac{n_{X=i}}{n_{total}}$
- Sometimes $p_{X=i}$ is shortened to $p_i$

**Relative frequency in the case of two variables:**


- $p_{X=i,Y=j} = \frac{n_{X=i,Y=j}}{n_{total}}$

## Marginal frequencies {.smaller}

The **marginal (or conditional) proportion** is the relative frequency of observations taking a certain value for a given variable (i.e., X), conditional on the observations taking a certain value for another variable (i.e., Y)

**Notation:**


- $p_{X=i \mid Y=j} = \frac{n_{X=i,\,Y=j}}{n_{Y=j}}$

- Sometimes $p_{X=i|Y=j}$ is shortened to $p_{ij}$

## Marginal frequency example {.smaller}

The table below is reconstructed from a published study:

|  | 18th Century | 19th Century |
|---|---|---|
| **Publishing Trade** | 109 | 21 |
| **Other Occupations** | 53 | 82 |
| **Total** | 162 | 103 |


::: {.panel-tabset}

### 18th Century
Marginal frequency of magazine founders' occupations being in the publishing trade, conditional on the magazine being founded in the 18th century: 

$$109/162 = 67.3\%$$

### 19th Century
Marginal frequency of magazine founders' occupations being in the publishing trade, conditional on the magazine being founded in the 19th century: 

$$21/103 = 20.3\%$$

::: 

::: aside
Source: Haveman, Habinek, and Goodman. 2012. "How Entrepreneurship Evolves: The Founders of New Magazines in America, 1741-1860." *Administrative Science Quarterly*. 57(4):553-584.
:::


## Back to relative frequencies… {.smaller}

We could use the table, which provides marginal frequencies, to calculate the relative frequency of magazine founders' occupations being in the publishing trade:

|  | 18th Century | 19th Century |
|---|---|---|
| **Publishing Trade** | 109 | 21 |
| **Other Occupations** | 53 | 82 |
| **Total** | 162 | 103 |

The number of all magazine foundings whose founders were in the publishing trades, divided by all magazine foundings:

$$(109 + 21)/(162 + 103) = 130/265 = 49.1\%$$

## Using relative & marginal frequencies {.smaller}

Relative frequencies are **more useful if both variables are categorical**, not continuous

- If the variable is continuous, there may not be very many observations that have a certain value for a given variable
- **Example**: yearly earnings (in dollars)—it would not be very informative to find the relative frequency of individuals with yearly earnings of $27,475

Same for marginal frequencies: more useful if both variables are categorical, not continuous

## Calculating frequencies in R {.smaller}

Now, let's use `R` to calculate relative and marginal frequencies
 
::: {.panel-tabset}

### Relative freq

Relative frequency formula : $p_{X=i} = \frac{n_{X=i}}{n_{total}}$



```{r}
#| echo: true
#| output-location: column

attain |> 
  # count absolute frequencies
  count(marital) |> 
  # create a proprotion  
  mutate(proportion = n / sum(n))

```

### Marginal freq (option 1)


Marginal frequency formula :  $p_{X=i \mid Y=j} = \frac{n_{X=i,\,Y=j}}{n_{Y=j}}$


```{r}
#| echo: true
#| output-location: column

attain |> 
  # Table of absolute freq for var1, 
  # for given value for var2
  filter(sex == "male") |> 
  count(marital) |> 
  # Use that table to create relative freq
  mutate(proportion = n / sum(n))

```

### Marginal freq (option 2)

Marginal frequency formula :  $p_{X=i \mid Y=j} = \frac{n_{X=i,\,Y=j}}{n_{Y=j}}$

```{r}
#| echo: true
#| output-location: column
#| 
attain |> 
  group_by(sex, marital) |> 
  summarize(n = n(), .groups = "drop_last") |> 
  mutate(proportion = n / sum(n))
```
:::

## Questions?


## Measures of central tendency {.smaller}

**Mode**: the value of a variable that occurs most often in the data set

**Median**: the middle data value, if the data are ordered by increasing value

**Mean** (aka average): the sum of all values in the data set, divided 

## Measures of central tendency {.smaller}

**Mode**: the value of a variable that occurs most often in the data set

Example data of number of people 10 members of a high school class sent text messages to Monday: `5, 2, 10, 4, 3, 2, 6, 15, 3, and 2`.

The mode of this variable in this data set is **2**, because it occurs more often (3 times) than any other value

## Measures of central tendency {.smaller}

**Median**: the middle data value, if the data are ordered by increasing value

- If there are an even number of observations, the median is the midpoint between the two middle values

## Measures of central tendency {.smaller}

Same example: (`5, 2, 10, 4, 3, 2, 6, 15, 3, 2`)

- First, order the data: (`2, 2, 2, 3, 3, 4, 5, 6, 10, 15`)

- Second, find the middle observation(s):  (`2, 2, 2, 3, 3, 4, 5, 6, 10, 15`)

Since we have two middle observations, find the midpoint: **3.5**

## Measures of central tendency {.smaller}

**(Arithmetic) mean** (aka average): the sum of all values in the data set, divided by the number of observations in the data set

**Notation:**

$$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$$

Example: (`5, 2, 10, 4, 3, 2, 6, 15, 3, 2`)

Mean = `(5+2+10+4+3+2+6+15+3+2)/10` = **5.2**

## Measures of central tendency {.smaller}

Example: (`5, 2, 10, 4, 3, 2, 6, 15, 3, 2`)

**Summary:**

- Mode = 2
- Median = 3.5
- Mean = 5.2

## A visual example 

::::: columns
::: {.column width="40%"}

<br>

Here's a visual example of how mode, mean, and median can work

:::

::: {.column width="60%"}
![](images/week4/histogram.png){width="100%,fig-align=\"center\""}
:::
:::::


## Comparing measures {.smaller}

::::: columns
::: {.column width="60%"}

- **Mode** can be used on variables of *any level of measurement*
  - Less useful for continuous variables, since values are not sufficiently granular (e.g. yearly earnings)
- **Median** *cannot be used on nominal variables*, because it requires you to order  observations
- **Mean** *cannot be used on nominal OR ordinal variables*, because it requires that we can say something about the distance between values

:::

::: {.column width="40%"}
| Measurement | Mode | Median | Mean |
|---|---|---|---|
| **Nominal** | Yes | No | No |
| **Ordinal** | Yes | Yes | No |
| **Continuous** | Yes | Yes | Yes |

:::
:::::


## Which should we preder? {.smaller}

Same example: `(2, 2, 2, 3, 3, 4, 5, 6, 10, 15)`. 

**Mode** is generally the least useful; in most cases we will prefer to use the median or mean

**Median** provides less information about the distribution of the variable, but is robust to outliers

- The median (3.5) does not depend on the values of the outliers 10 and 15

**Mean** takes into account the distribution of the variable, but is less robust to outliers

- The mean (5.2) is strongly affected by the values of the outliers 10 and 15

## Measures of central tendency in R {.smaller}

These measures are simple to calculate in `R`. Let's take a look: 

::: {.panel-tabset}

### Mean

To calculate the mean respondent income in our sample, we can use the `summarize()` or `summarise()` function. Be sure to remove missing using `na.rm = TRUE`, which can affect the calculation. 

```{r}
#| echo: true
#| output-location: column

# Mean
attain |> 
  summarise(mean = mean(rincom91, na.rm = TRUE)) |> 
  print()

```


### Meidan

We can do the same with for the median using the `median()` function: 

```{r}
#| echo: true
#| output-location: column

# Median
attain |> 
  summarise(mean = median(rincom91, na.rm = TRUE)) |> 
  print()

```

### Mode

For the mode, we'll simply want to count the number of obs (using `count()`) and take the largest number. We can do following to see the most common martial status, which is unsurprising "married".

::: aside
Notice the extra `glimpse()` call here to see what's happening under the hood
:::

```{r}
#| echo: true
#| output-location: column

attain %>%
  count(marital) %>%
  glimpse() |> 
  slice_max(n, n = 1, with_ties = FALSE) %>%
  pull(marital)

```

:::

## Questions?

## Measures of dispersion {.smaller}

- **Percentiles**: the $k$th percentile of the variable is the value that has the property that at least $k$ percent of the data are less than or equal to it

- **Deviation from the mean**: the difference between a data value and the sample mean

- **Sample variance**: the average squared deviations

- **Sample standard deviation**: the square root of the sample variance


## Measures of dispersion {.smaller}

**Percentiles**: the $k$th percentile of the variable is the value that has the property that at least k percent of the data are less than or equal to it

**Calculating the $k$th percentile:**

1. Order the data from smallest to largest: $(x_1, ..., x_n)$
2. Calculate the index $i = \frac{k \times n_{total}}{100}$
3. If index is a whole number, the percentile is an average of the ith and the ($i$+1)th values in the ordered data
4. If index is not a whole number, round up to whole number, and the percentile is the $i$th number

## Measures of dispersion {.smaller}

**Example**: calculating the 30th and 45th percentile of the dataset of # texts per day: `(5, 2, 10, 4, 3, 2, 6, 15, 3, 2)`

First, order the data: **(2, 2, 2, 3, 3, 4, 5, 6, 10, 15)**

Second, calculate the indexes:

- 30th: $30 \times 10/100 = 3$
- 45th: $45 \times 10/100 = 4.5$

For the **30th percentile**: the index 3 is a whole number, so we take the average of the 3rd and 4th values in the ordered dataset: $(2 + 3)/2 = 2.5$

For the **45th percentile**: the index 4.5 is not a whole number, so we round up to 5 and take the 5th value in the ordered dataset: **3**

## Measures of dispersion {.smaller}

**Deviation from the mean**: the difference between a data value and the sample mean

For observation $i$, the value of the ith deviation is: $x_i - \bar{x}$

We often use the squared deviations from the mean to calculate the dispersion of the data:

**Sample variance**: the average* squared deviations

$$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$$

*Note that these "averages" are divided by n-1, not n.

## Measures of dispersion {.smaller}

**Sample standard deviation**: the square root of the sample variance

$$s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}$$

## Measures of dispersion {.smaller}

**Why do we square the deviations?**

- Ensures that the deviations will always be positive
  - But, using the absolute value could do the same
- Gives greater weight to outliers
  - This has good and bad properties

**Unsatisfying answer**: most of the statistical tools we will learn in the rest of the course are based on the standard deviation and variance, and these are the most commonly used measures of dispersion

## Comparing measures of dispersion {.smaller}

The tradeoff between percentiles and standard deviations / variances is similar to the tradeoff between medians and means

- **Percentiles** only provide information about the distribution of the variable at one point, but are fairly robust to outliers
  - Like the median, which is the same as the 50th percentile
- The standard deviation / variance uses information about the entire distribution of the variable, but is not as robust to outliers


## Dispersion and levels of measurement {.smaller}

So, which levels of measurement can use percentiles and variance/standard deviation?

<br>

| Statistic          | Nominal | Ordinal | Continuous | Why                      |
|--------------------|---------|---------|-----------|---------------------------|
| Percentiles        | ❌ No    | ✅ Yes  | ✅ Yes    | Percentiles require at least ordered data |
| Variance / Std Dev | ❌ No    | ❌ No   | ✅ Yes    | Requires meaningful distances between values |


## Measures of dispersion in R {.smaller}

How do we use R to calculate measures of dispersion? Let's look at variosu measures of dispersion of respondent's income in our `attain` dataset?

::: {.panel-tabset}

### Percentiles

This will return the values at those specific **percentiles**--25th, 50th, and 75th. So, a respondent with an income of \$32,500 is in the 75th percentile of the sample. 

```{r}
#| echo: true
#| output-location: column

# Percentiles
attain %>%
  summarise(
    p25 = quantile(rincom91, 0.25, na.rm = TRUE),
    p50 = quantile(rincom91, 0.50, na.rm = TRUE),
    p75 = quantile(rincom91, 0.75, na.rm = TRUE)
  ) |> 
  glimpse()

```

### Variance

This will return the measure of **variance** in our sample for this specific income variable.

```{r}
#| echo: true
#| output-location: column

# Variance
attain %>%
  summarise(variance = var(rincom91, na.rm = TRUE)) |> 
  glimpse()

```

### Standard Deviation

This will return the **standard deviation (std_dev)** of variance in our sample for this specific income variable.

```{r}
#| echo: true
#| output-location: column

# Standard deviation
attain %>%
  summarise(std_dev = sd(rincom91, na.rm = TRUE)) |> 
  glimpse()

```

### All at once

we can calculate all of these measures simultaneously. We can also calculate `sd` manually if we'd like. 

<br>

Here I use the `gt` package to make the table output a bit nicer. [Check out the documentation](https://gt.rstudio.com)!

```{r}
#| echo: true
#| output-location: column

# Verify sd equals sqrt(var)
attain %>%
  summarise(
    variance = var(rincom91, na.rm = TRUE),
    std_dev  = sd(rincom91, na.rm = TRUE),
    sqrt_var = sqrt(var(rincom91, na.rm = TRUE))
  ) |> 
  # use `gt` for nicer table formatting!
  gt::gt()

```

::: 

## Questions?

## Measures of association {.smaller}

**Differences in proportions**: the difference in marginal proportions for the same outcome variable ($x_i$) across different categories of a variable y

Formula: $p_{x_i | y_j} - p_{x_i | y_k}$

- In words: "The difference in the proportion of $x_i$ between group $j$ and group $k$"
- The differences in proportions are expressed in terms of percentage points
- Easy to use, but occasionally misleading

> Also, remember there is big difference in "percentage point increase" and "percent increase"

## Measures of association {.smaller}

**Relative risk ratio**: ratio of marginal proportions for the same outcome variable (x = i) across different categories of a variable y

Formula: $p_{x_i | y_j} / p_{x_i | y_k}$

- Scales the proportions against each other (which can be generally less misleading than simple differences)

## Comparing proportions: example {.smaller}

|  | 18th Century | 19th Century |
|---|---|---|
| **Publishing Trade** | 109 | 21 |
| **Other Occupations** | 53 | 82 |
| **Total** | 162 | 103 |


::: {.panel-tabset}

### Difference in proportions

**Difference in proportions** of a magazine founder being in the publishing trade (x) in the 18th as compared to the 19th century (y):

$$109/162 - 21/103 = 67.3\% - 20.3\% = 47 \text{ percentage points}$$

This is an absolute difference—it tells you the raw gap between the two proportions.

> "Magazine founders in the 18th century were 47 percentage points more likely to be in the publishing trade than those in the 19th century." 

### Relative risk ratios

**Relative risk ratio** of a magazine founder being in the publishing trade (x) in the 18th as compared to the 19th century (y):

$$(109/162)/(21/103) = 67.3\%/20.3\% = 3.32$$

This is a relative difference—how many times larger one proportion is than another.

> "Magazine founders in the 18th century were 3.32 times as likely (or 232% more likely) to be in the publishing trade compared to those in the 19th century."

::: 

## Comparing proportions: example {.smaller}


**Which One to Use?**

- **Absolute difference** (47 pp): Better for understanding practical/real-world impact. Nearly half of 18th century founders were publishers vs. only a fifth in the 19th century—that's a massive shift in the composition of magazine founders.

- **Relative risk** (3.32x): Better for understanding the strength of the association. Being an 18th century founder makes you over 3 times as likely to be a publisher.

## Comparing variables across the distribution {.smaller}

**The covariance** for $n$ observations on two variables $x$ and $y$ is equal to:

$$cov(x,y) = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$

Covariance measures whether $x$ and $y$ tend to move together or not

Note that the term $(x_i - \bar{x})(y_i - \bar{y})$ is positive for observation $i$ if $x_i$ and $y_i$ are both below or both above the means of $x$ and $y$, and negative if not

## Comparing variables across the distribution {.smaller}

**The correlation coefficient** $r$ for $n$ observations on two variables x and y with sample standard deviations $s_x$ and $s_y$ is equal to:

$$r_{xy} = \frac{cov(x,y)}{s_x \times s_y}$$

## Correlation coefficient properties  {.smaller}

- $r_{xy}$ is always between -1 and 1
- $r_{xy}$ equals 1 if all observations fall along a straight line that has a positive slope (that is, as x increases, y increases in direct proportion)
- $r_{xy}$ equals -1 if all observations fall along a straight line that has a negative slope (that is, as x increases, y decreases in direct proportion)
- $r_{xy}$ does not depend on the scale of either $x$ and $y$

## Correlation coefficient properties  {.smaller}

**If $r_{xy}$ equals 0, this does not imply that there is no relationship between $x$ and $y$**

It only implies that there is no linear relationship between $x$ and $y$

**Always use a scatterplot to check!**

## Correlation examples 

Here are some examples of different correlation strengths might look like in terms of data

![](images/week4/correlation_examples.png){width="100%" fig-align="center"}


## Other correlation coefficients {.smaller}

$r_{xy}$ (also known as Pearson's correlation coefficient) can only be computed for two continuous variables

Other measures with similar properties are available for ordinal variables:

- **Polychoric correlation**: two ordinal variables
- **Polyserial correlation**: one ordinal variable and one continuous variable

> We won't cover these more in class becuase I find them to be rarely used in practice

## Which measures of association to use? {.smaller}

|  | **Nominal** | **Ordinal** | **Continuous** |
|---|---|---|---|
| **Nominal** | Difference in proportions; relative risk ratio | Difference in proportions; relative risk ratio | Difference in means between groups |
| **Ordinal** |  | Polychoric correlation | Difference in means between groups; polyserial correlation |
| **Continuous** |  |  | Covariance, (Pearson's) correlation coefficient |

## Measures of association in R {.smaller}

::: {.panel-tabset}

### Proportions

**Difference in proportions, relative risk ratio:**

Use the results of the marginal frequency tables

### Covariance 

```{r}
#| echo: true

# Covariance
attain %>% 
  summarize(covariance = cov(rincom91, income91, use = "complete.obs")) |> 
  glimpse()

```


### Correlation

```{r}
#| echo: true

# Correlation coefficient
attain %>%
  summarize(correlation = cor(rincom91, income91, use = "complete.obs"))

```
:::

## Questions?

## Weekly Assignment #3 {.smaller}

**Two-part assignment:**

1. Practice summarizing variables from attain.csv (from the first homework assignment): see Word document on bCourses

2. Answer a research question relating to the association between two variables in a data set of your choosing, using a proper measure of association for the levels of measurement of the variables. Write up your results according to the "Weekly Assignment Stata Analysis" template.

**HW #3:** due Thursday
 
 - any questions?